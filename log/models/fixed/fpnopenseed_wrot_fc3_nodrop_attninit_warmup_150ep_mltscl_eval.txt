[2023-06-26 17:33:59,195] DATA:
  IMGS_DIR: /srv/datasets2/nutrition5k_dataset/imagery/realsense_overhead
  METADATAS_PATH: /srv/datasets2/nutrition5k_dataset/metadata/dish_metadata_cafe1.csv
  NAME: nutrition5k
  SPLITS_TEST_PATH: /srv/datasets2/nutrition5k_dataset/dish_ids/splits/depth_test_ids.txt
  SPLITS_TRAIN_PATH: /srv/datasets2/nutrition5k_dataset/dish_ids/splits/depth_train_ids.txt
EVAL:
  HEIGHT: 384
  WIDTH: 512
MODEL:
  MASK_WEIGHT: 0.5
  NAME: openseed
  PRETRAINED: ''
SAVE_PATH: models/fixed/fpnopenseed_wrot_fc3_nodrop_attninit_warmup_150ep_mltscl.pt
TITLE:
- fpn openseed with aug+rotate fc 3 no dropout freeze new attn initialize with warmup
  150ep multi scale
TRAIN:
  BATCH_SIZE: 32
  CKPT: null
  FINETUNE: false
  LAYERS: 1
  LOSS: multi
  LR: 5.0e-05
  NUM_EPOCHS: 150
  SEED: 12345
  WEIGHT_DECAY: 0.0001

[2023-06-26 17:33:59,982] URL https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth cached in /home/parinayok/.torch/iopath_cache/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth
[2023-06-26 17:34:02,714] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,717] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,717] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,723] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,723] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,729] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,729] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,731] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,731] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,737] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,737] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,743] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,743] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,744] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,744] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,750] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,750] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,756] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,756] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,758] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,758] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,764] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,764] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,770] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,770] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,772] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,772] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,778] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,778] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,784] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,784] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,785] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,785] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,791] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,791] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,797] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,797] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,799] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,799] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,805] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,805] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,811] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,811] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,813] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,813] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,819] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,819] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,825] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,825] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,826] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,826] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,832] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,832] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,838] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,838] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,840] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,840] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,846] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,846] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,852] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,852] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,854] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,854] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,860] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,860] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,866] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,866] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,867] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,867] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,873] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:02,873] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:02,880] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:03,546] Loaded backbone.layers.0.blocks.0.attn.proj.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:03,546] Loaded backbone.layers.0.blocks.0.attn.proj.weight, Model Shape: torch.Size([96, 96]) <-> Ckpt Shape: torch.Size([96, 96])
[2023-06-26 17:34:03,546] Loaded backbone.layers.0.blocks.0.attn.qkv.bias, Model Shape: torch.Size([288]) <-> Ckpt Shape: torch.Size([288])
[2023-06-26 17:34:03,546] Loaded backbone.layers.0.blocks.0.attn.qkv.weight, Model Shape: torch.Size([288, 96]) <-> Ckpt Shape: torch.Size([288, 96])
[2023-06-26 17:34:03,546] Loaded backbone.layers.0.blocks.0.attn.relative_position_bias_table, Model Shape: torch.Size([169, 3]) <-> Ckpt Shape: torch.Size([169, 3])
[2023-06-26 17:34:03,546] Loaded backbone.layers.0.blocks.0.attn.relative_position_index, Model Shape: torch.Size([49, 49]) <-> Ckpt Shape: torch.Size([49, 49])
[2023-06-26 17:34:03,546] Loaded backbone.layers.0.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,546] Loaded backbone.layers.0.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([384, 96]) <-> Ckpt Shape: torch.Size([384, 96])
[2023-06-26 17:34:03,546] Loaded backbone.layers.0.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:03,546] Loaded backbone.layers.0.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([96, 384]) <-> Ckpt Shape: torch.Size([96, 384])
[2023-06-26 17:34:03,546] Loaded backbone.layers.0.blocks.0.norm1.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:03,546] Loaded backbone.layers.0.blocks.0.norm1.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:03,546] Loaded backbone.layers.0.blocks.0.norm2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:03,546] Loaded backbone.layers.0.blocks.0.norm2.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:03,547] Loaded backbone.layers.0.blocks.1.attn.proj.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:03,547] Loaded backbone.layers.0.blocks.1.attn.proj.weight, Model Shape: torch.Size([96, 96]) <-> Ckpt Shape: torch.Size([96, 96])
[2023-06-26 17:34:03,547] Loaded backbone.layers.0.blocks.1.attn.qkv.bias, Model Shape: torch.Size([288]) <-> Ckpt Shape: torch.Size([288])
[2023-06-26 17:34:03,547] Loaded backbone.layers.0.blocks.1.attn.qkv.weight, Model Shape: torch.Size([288, 96]) <-> Ckpt Shape: torch.Size([288, 96])
[2023-06-26 17:34:03,547] Loaded backbone.layers.0.blocks.1.attn.relative_position_bias_table, Model Shape: torch.Size([169, 3]) <-> Ckpt Shape: torch.Size([169, 3])
[2023-06-26 17:34:03,547] Loaded backbone.layers.0.blocks.1.attn.relative_position_index, Model Shape: torch.Size([49, 49]) <-> Ckpt Shape: torch.Size([49, 49])
[2023-06-26 17:34:03,547] Loaded backbone.layers.0.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,547] Loaded backbone.layers.0.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([384, 96]) <-> Ckpt Shape: torch.Size([384, 96])
[2023-06-26 17:34:03,547] Loaded backbone.layers.0.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:03,547] Loaded backbone.layers.0.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([96, 384]) <-> Ckpt Shape: torch.Size([96, 384])
[2023-06-26 17:34:03,547] Loaded backbone.layers.0.blocks.1.norm1.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:03,547] Loaded backbone.layers.0.blocks.1.norm1.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:03,547] Loaded backbone.layers.0.blocks.1.norm2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:03,547] Loaded backbone.layers.0.blocks.1.norm2.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:03,547] Loaded backbone.layers.0.downsample.norm.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,547] Loaded backbone.layers.0.downsample.norm.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,547] Loaded backbone.layers.0.downsample.reduction.weight, Model Shape: torch.Size([192, 384]) <-> Ckpt Shape: torch.Size([192, 384])
[2023-06-26 17:34:03,547] Loaded backbone.layers.1.blocks.0.attn.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:03,547] Loaded backbone.layers.1.blocks.0.attn.proj.weight, Model Shape: torch.Size([192, 192]) <-> Ckpt Shape: torch.Size([192, 192])
[2023-06-26 17:34:03,547] Loaded backbone.layers.1.blocks.0.attn.qkv.bias, Model Shape: torch.Size([576]) <-> Ckpt Shape: torch.Size([576])
[2023-06-26 17:34:03,547] Loaded backbone.layers.1.blocks.0.attn.qkv.weight, Model Shape: torch.Size([576, 192]) <-> Ckpt Shape: torch.Size([576, 192])
[2023-06-26 17:34:03,547] Loaded backbone.layers.1.blocks.0.attn.relative_position_bias_table, Model Shape: torch.Size([169, 6]) <-> Ckpt Shape: torch.Size([169, 6])
[2023-06-26 17:34:03,547] Loaded backbone.layers.1.blocks.0.attn.relative_position_index, Model Shape: torch.Size([49, 49]) <-> Ckpt Shape: torch.Size([49, 49])
[2023-06-26 17:34:03,547] Loaded backbone.layers.1.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,547] Loaded backbone.layers.1.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])
[2023-06-26 17:34:03,547] Loaded backbone.layers.1.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:03,547] Loaded backbone.layers.1.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])
[2023-06-26 17:34:03,547] Loaded backbone.layers.1.blocks.0.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:03,547] Loaded backbone.layers.1.blocks.0.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:03,548] Loaded backbone.layers.1.blocks.0.norm2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:03,548] Loaded backbone.layers.1.blocks.0.norm2.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:03,548] Loaded backbone.layers.1.blocks.1.attn.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:03,548] Loaded backbone.layers.1.blocks.1.attn.proj.weight, Model Shape: torch.Size([192, 192]) <-> Ckpt Shape: torch.Size([192, 192])
[2023-06-26 17:34:03,548] Loaded backbone.layers.1.blocks.1.attn.qkv.bias, Model Shape: torch.Size([576]) <-> Ckpt Shape: torch.Size([576])
[2023-06-26 17:34:03,548] Loaded backbone.layers.1.blocks.1.attn.qkv.weight, Model Shape: torch.Size([576, 192]) <-> Ckpt Shape: torch.Size([576, 192])
[2023-06-26 17:34:03,548] Loaded backbone.layers.1.blocks.1.attn.relative_position_bias_table, Model Shape: torch.Size([169, 6]) <-> Ckpt Shape: torch.Size([169, 6])
[2023-06-26 17:34:03,548] Loaded backbone.layers.1.blocks.1.attn.relative_position_index, Model Shape: torch.Size([49, 49]) <-> Ckpt Shape: torch.Size([49, 49])
[2023-06-26 17:34:03,548] Loaded backbone.layers.1.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,548] Loaded backbone.layers.1.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])
[2023-06-26 17:34:03,548] Loaded backbone.layers.1.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:03,548] Loaded backbone.layers.1.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])
[2023-06-26 17:34:03,548] Loaded backbone.layers.1.blocks.1.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:03,548] Loaded backbone.layers.1.blocks.1.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:03,548] Loaded backbone.layers.1.blocks.1.norm2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:03,548] Loaded backbone.layers.1.blocks.1.norm2.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:03,548] Loaded backbone.layers.1.downsample.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,548] Loaded backbone.layers.1.downsample.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,548] Loaded backbone.layers.1.downsample.reduction.weight, Model Shape: torch.Size([384, 768]) <-> Ckpt Shape: torch.Size([384, 768])
[2023-06-26 17:34:03,548] Loaded backbone.layers.2.blocks.0.attn.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,548] Loaded backbone.layers.2.blocks.0.attn.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])
[2023-06-26 17:34:03,548] Loaded backbone.layers.2.blocks.0.attn.qkv.bias, Model Shape: torch.Size([1152]) <-> Ckpt Shape: torch.Size([1152])
[2023-06-26 17:34:03,548] Loaded backbone.layers.2.blocks.0.attn.qkv.weight, Model Shape: torch.Size([1152, 384]) <-> Ckpt Shape: torch.Size([1152, 384])
[2023-06-26 17:34:03,548] Loaded backbone.layers.2.blocks.0.attn.relative_position_bias_table, Model Shape: torch.Size([169, 12]) <-> Ckpt Shape: torch.Size([169, 12])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.0.attn.relative_position_index, Model Shape: torch.Size([49, 49]) <-> Ckpt Shape: torch.Size([49, 49])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.0.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.0.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.0.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.0.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.1.attn.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.1.attn.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.1.attn.qkv.bias, Model Shape: torch.Size([1152]) <-> Ckpt Shape: torch.Size([1152])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.1.attn.qkv.weight, Model Shape: torch.Size([1152, 384]) <-> Ckpt Shape: torch.Size([1152, 384])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.1.attn.relative_position_bias_table, Model Shape: torch.Size([169, 12]) <-> Ckpt Shape: torch.Size([169, 12])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.1.attn.relative_position_index, Model Shape: torch.Size([49, 49]) <-> Ckpt Shape: torch.Size([49, 49])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.1.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.1.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.1.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.1.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.2.attn.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.2.attn.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.2.attn.qkv.bias, Model Shape: torch.Size([1152]) <-> Ckpt Shape: torch.Size([1152])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.2.attn.qkv.weight, Model Shape: torch.Size([1152, 384]) <-> Ckpt Shape: torch.Size([1152, 384])
[2023-06-26 17:34:03,549] Loaded backbone.layers.2.blocks.2.attn.relative_position_bias_table, Model Shape: torch.Size([169, 12]) <-> Ckpt Shape: torch.Size([169, 12])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.2.attn.relative_position_index, Model Shape: torch.Size([49, 49]) <-> Ckpt Shape: torch.Size([49, 49])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.2.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.2.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.2.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.2.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.2.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.2.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.2.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.2.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.3.attn.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.3.attn.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.3.attn.qkv.bias, Model Shape: torch.Size([1152]) <-> Ckpt Shape: torch.Size([1152])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.3.attn.qkv.weight, Model Shape: torch.Size([1152, 384]) <-> Ckpt Shape: torch.Size([1152, 384])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.3.attn.relative_position_bias_table, Model Shape: torch.Size([169, 12]) <-> Ckpt Shape: torch.Size([169, 12])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.3.attn.relative_position_index, Model Shape: torch.Size([49, 49]) <-> Ckpt Shape: torch.Size([49, 49])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.3.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.3.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.3.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.3.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.3.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.3.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.3.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.3.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.4.attn.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.4.attn.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.4.attn.qkv.bias, Model Shape: torch.Size([1152]) <-> Ckpt Shape: torch.Size([1152])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.4.attn.qkv.weight, Model Shape: torch.Size([1152, 384]) <-> Ckpt Shape: torch.Size([1152, 384])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.4.attn.relative_position_bias_table, Model Shape: torch.Size([169, 12]) <-> Ckpt Shape: torch.Size([169, 12])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.4.attn.relative_position_index, Model Shape: torch.Size([49, 49]) <-> Ckpt Shape: torch.Size([49, 49])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.4.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.4.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])
[2023-06-26 17:34:03,550] Loaded backbone.layers.2.blocks.4.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,551] Loaded backbone.layers.2.blocks.4.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])
[2023-06-26 17:34:03,551] Loaded backbone.layers.2.blocks.4.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,551] Loaded backbone.layers.2.blocks.4.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,551] Loaded backbone.layers.2.blocks.4.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,551] Loaded backbone.layers.2.blocks.4.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,551] Loaded backbone.layers.2.blocks.5.attn.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,551] Loaded backbone.layers.2.blocks.5.attn.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])
[2023-06-26 17:34:03,551] Loaded backbone.layers.2.blocks.5.attn.qkv.bias, Model Shape: torch.Size([1152]) <-> Ckpt Shape: torch.Size([1152])
[2023-06-26 17:34:03,551] Loaded backbone.layers.2.blocks.5.attn.qkv.weight, Model Shape: torch.Size([1152, 384]) <-> Ckpt Shape: torch.Size([1152, 384])
[2023-06-26 17:34:03,551] Loaded backbone.layers.2.blocks.5.attn.relative_position_bias_table, Model Shape: torch.Size([169, 12]) <-> Ckpt Shape: torch.Size([169, 12])
[2023-06-26 17:34:03,551] Loaded backbone.layers.2.blocks.5.attn.relative_position_index, Model Shape: torch.Size([49, 49]) <-> Ckpt Shape: torch.Size([49, 49])
[2023-06-26 17:34:03,551] Loaded backbone.layers.2.blocks.5.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:03,551] Loaded backbone.layers.2.blocks.5.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])
[2023-06-26 17:34:03,551] Loaded backbone.layers.2.blocks.5.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,551] Loaded backbone.layers.2.blocks.5.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])
[2023-06-26 17:34:03,551] Loaded backbone.layers.2.blocks.5.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,551] Loaded backbone.layers.2.blocks.5.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,551] Loaded backbone.layers.2.blocks.5.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,551] Loaded backbone.layers.2.blocks.5.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,551] Loaded backbone.layers.2.downsample.norm.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:03,551] Loaded backbone.layers.2.downsample.norm.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:03,551] Loaded backbone.layers.2.downsample.reduction.weight, Model Shape: torch.Size([768, 1536]) <-> Ckpt Shape: torch.Size([768, 1536])
[2023-06-26 17:34:03,551] Loaded backbone.layers.3.blocks.0.attn.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,551] Loaded backbone.layers.3.blocks.0.attn.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])
[2023-06-26 17:34:03,551] Loaded backbone.layers.3.blocks.0.attn.qkv.bias, Model Shape: torch.Size([2304]) <-> Ckpt Shape: torch.Size([2304])
[2023-06-26 17:34:03,551] Loaded backbone.layers.3.blocks.0.attn.qkv.weight, Model Shape: torch.Size([2304, 768]) <-> Ckpt Shape: torch.Size([2304, 768])
[2023-06-26 17:34:03,552] Loaded backbone.layers.3.blocks.0.attn.relative_position_bias_table, Model Shape: torch.Size([169, 24]) <-> Ckpt Shape: torch.Size([169, 24])
[2023-06-26 17:34:03,552] Loaded backbone.layers.3.blocks.0.attn.relative_position_index, Model Shape: torch.Size([49, 49]) <-> Ckpt Shape: torch.Size([49, 49])
[2023-06-26 17:34:03,552] Loaded backbone.layers.3.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])
[2023-06-26 17:34:03,552] Loaded backbone.layers.3.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])
[2023-06-26 17:34:03,552] Loaded backbone.layers.3.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,552] Loaded backbone.layers.3.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])
[2023-06-26 17:34:03,552] Loaded backbone.layers.3.blocks.0.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,552] Loaded backbone.layers.3.blocks.0.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,552] Loaded backbone.layers.3.blocks.0.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,552] Loaded backbone.layers.3.blocks.0.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,552] Loaded backbone.layers.3.blocks.1.attn.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,552] Loaded backbone.layers.3.blocks.1.attn.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])
[2023-06-26 17:34:03,552] Loaded backbone.layers.3.blocks.1.attn.qkv.bias, Model Shape: torch.Size([2304]) <-> Ckpt Shape: torch.Size([2304])
[2023-06-26 17:34:03,552] Loaded backbone.layers.3.blocks.1.attn.qkv.weight, Model Shape: torch.Size([2304, 768]) <-> Ckpt Shape: torch.Size([2304, 768])
[2023-06-26 17:34:03,552] Loaded backbone.layers.3.blocks.1.attn.relative_position_bias_table, Model Shape: torch.Size([169, 24]) <-> Ckpt Shape: torch.Size([169, 24])
[2023-06-26 17:34:03,552] Loaded backbone.layers.3.blocks.1.attn.relative_position_index, Model Shape: torch.Size([49, 49]) <-> Ckpt Shape: torch.Size([49, 49])
[2023-06-26 17:34:03,552] Loaded backbone.layers.3.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])
[2023-06-26 17:34:03,552] Loaded backbone.layers.3.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])
[2023-06-26 17:34:03,552] Loaded backbone.layers.3.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,552] Loaded backbone.layers.3.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])
[2023-06-26 17:34:03,552] Loaded backbone.layers.3.blocks.1.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,552] Loaded backbone.layers.3.blocks.1.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,552] Loaded backbone.layers.3.blocks.1.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,552] Loaded backbone.layers.3.blocks.1.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,552] Loaded backbone.norm0.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:03,552] Loaded backbone.norm0.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:03,552] Loaded backbone.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:03,553] Loaded backbone.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:03,553] Loaded backbone.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,553] Loaded backbone.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:03,553] Loaded backbone.norm3.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,553] Loaded backbone.norm3.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,553] Loaded backbone.patch_embed.norm.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:03,553] Loaded backbone.patch_embed.norm.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:03,553] Loaded backbone.patch_embed.proj.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:03,553] Loaded backbone.patch_embed.proj.weight, Model Shape: torch.Size([96, 3, 4, 4]) <-> Ckpt Shape: torch.Size([96, 3, 4, 4])
[2023-06-26 17:34:03,553] Loaded sem_seg_head.pixel_decoder.adapter_1.norm.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,553] Loaded sem_seg_head.pixel_decoder.adapter_1.norm.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,553] Loaded sem_seg_head.pixel_decoder.adapter_1.weight, Model Shape: torch.Size([256, 96, 1, 1]) <-> Ckpt Shape: torch.Size([256, 96, 1, 1])
[2023-06-26 17:34:03,553] Loaded sem_seg_head.pixel_decoder.input_proj.0.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,553] Loaded sem_seg_head.pixel_decoder.input_proj.0.0.weight, Model Shape: torch.Size([256, 192, 1, 1]) <-> Ckpt Shape: torch.Size([256, 192, 1, 1])
[2023-06-26 17:34:03,553] Loaded sem_seg_head.pixel_decoder.input_proj.0.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,553] Loaded sem_seg_head.pixel_decoder.input_proj.0.1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,553] Loaded sem_seg_head.pixel_decoder.input_proj.1.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,553] Loaded sem_seg_head.pixel_decoder.input_proj.1.0.weight, Model Shape: torch.Size([256, 384, 1, 1]) <-> Ckpt Shape: torch.Size([256, 384, 1, 1])
[2023-06-26 17:34:03,553] Loaded sem_seg_head.pixel_decoder.input_proj.1.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,553] Loaded sem_seg_head.pixel_decoder.input_proj.1.1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,553] Loaded sem_seg_head.pixel_decoder.input_proj.2.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,553] Loaded sem_seg_head.pixel_decoder.input_proj.2.0.weight, Model Shape: torch.Size([256, 768, 1, 1]) <-> Ckpt Shape: torch.Size([256, 768, 1, 1])
[2023-06-26 17:34:03,553] Loaded sem_seg_head.pixel_decoder.input_proj.2.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,553] Loaded sem_seg_head.pixel_decoder.input_proj.2.1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,553] Loaded sem_seg_head.pixel_decoder.input_proj.3.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,553] Loaded sem_seg_head.pixel_decoder.input_proj.3.0.weight, Model Shape: torch.Size([256, 768, 3, 3]) <-> Ckpt Shape: torch.Size([256, 768, 3, 3])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.input_proj.3.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.input_proj.3.1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.layer_1.norm.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.layer_1.norm.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.layer_1.weight, Model Shape: torch.Size([256, 256, 3, 3]) <-> Ckpt Shape: torch.Size([256, 256, 3, 3])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.mask_features.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.mask_features.weight, Model Shape: torch.Size([256, 256, 1, 1]) <-> Ckpt Shape: torch.Size([256, 256, 1, 1])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:03,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:03,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:03,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:03,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:03,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,556] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:03,556] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:03,556] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,556] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,556] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,556] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,556] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,556] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,556] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,556] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:03,556] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,556] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:03,556] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,556] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,557] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,557] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,557] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:03,557] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:03,557] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,557] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,557] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,557] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,557] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,557] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,557] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,557] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:03,557] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,557] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:03,557] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,557] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,557] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,558] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,558] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:03,558] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:03,558] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,558] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,558] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,558] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,559] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,559] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,559] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,559] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:03,559] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,559] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:03,559] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,559] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,559] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,560] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,560] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:03,560] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:03,560] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,560] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,560] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,560] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,560] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,560] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,560] Loaded sem_seg_head.pixel_decoder.transformer.level_embed, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:03,560] Loaded sem_seg_head.predictor._bbox_embed.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,561] Loaded sem_seg_head.predictor._bbox_embed.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,561] Loaded sem_seg_head.predictor._bbox_embed.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,561] Loaded sem_seg_head.predictor._bbox_embed.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,561] Loaded sem_seg_head.predictor._bbox_embed.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:03,561] Loaded sem_seg_head.predictor._bbox_embed.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:03,561] Loaded sem_seg_head.predictor.bbox_embed.0.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,561] Loaded sem_seg_head.predictor.bbox_embed.0.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,561] Loaded sem_seg_head.predictor.bbox_embed.0.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,561] Loaded sem_seg_head.predictor.bbox_embed.0.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,561] Loaded sem_seg_head.predictor.bbox_embed.0.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:03,561] Loaded sem_seg_head.predictor.bbox_embed.0.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:03,561] Loaded sem_seg_head.predictor.bbox_embed.1.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,561] Loaded sem_seg_head.predictor.bbox_embed.1.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,561] Loaded sem_seg_head.predictor.bbox_embed.1.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,562] Loaded sem_seg_head.predictor.bbox_embed.1.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,562] Loaded sem_seg_head.predictor.bbox_embed.1.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:03,562] Loaded sem_seg_head.predictor.bbox_embed.1.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:03,562] Loaded sem_seg_head.predictor.bbox_embed.2.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,562] Loaded sem_seg_head.predictor.bbox_embed.2.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,562] Loaded sem_seg_head.predictor.bbox_embed.2.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,562] Loaded sem_seg_head.predictor.bbox_embed.2.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,562] Loaded sem_seg_head.predictor.bbox_embed.2.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:03,562] Loaded sem_seg_head.predictor.bbox_embed.2.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:03,562] Loaded sem_seg_head.predictor.bbox_embed.3.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,562] Loaded sem_seg_head.predictor.bbox_embed.3.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,562] Loaded sem_seg_head.predictor.bbox_embed.3.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,562] Loaded sem_seg_head.predictor.bbox_embed.3.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,562] Loaded sem_seg_head.predictor.bbox_embed.3.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:03,562] Loaded sem_seg_head.predictor.bbox_embed.3.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:03,562] Loaded sem_seg_head.predictor.bbox_embed.4.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,562] Loaded sem_seg_head.predictor.bbox_embed.4.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,563] Loaded sem_seg_head.predictor.bbox_embed.4.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,563] Loaded sem_seg_head.predictor.bbox_embed.4.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,563] Loaded sem_seg_head.predictor.bbox_embed.4.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:03,563] Loaded sem_seg_head.predictor.bbox_embed.4.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:03,563] Loaded sem_seg_head.predictor.bbox_embed.5.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,563] Loaded sem_seg_head.predictor.bbox_embed.5.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,563] Loaded sem_seg_head.predictor.bbox_embed.5.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,563] Loaded sem_seg_head.predictor.bbox_embed.5.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,563] Loaded sem_seg_head.predictor.bbox_embed.5.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:03,563] Loaded sem_seg_head.predictor.bbox_embed.5.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:03,563] Loaded sem_seg_head.predictor.bbox_embed.6.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,563] Loaded sem_seg_head.predictor.bbox_embed.6.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,563] Loaded sem_seg_head.predictor.bbox_embed.6.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,563] Loaded sem_seg_head.predictor.bbox_embed.6.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,563] Loaded sem_seg_head.predictor.bbox_embed.6.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:03,563] Loaded sem_seg_head.predictor.bbox_embed.6.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:03,563] Loaded sem_seg_head.predictor.bbox_embed.7.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,563] Loaded sem_seg_head.predictor.bbox_embed.7.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,563] Loaded sem_seg_head.predictor.bbox_embed.7.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,564] Loaded sem_seg_head.predictor.bbox_embed.7.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,564] Loaded sem_seg_head.predictor.bbox_embed.7.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:03,564] Loaded sem_seg_head.predictor.bbox_embed.7.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:03,564] Loaded sem_seg_head.predictor.bbox_embed.8.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,564] Loaded sem_seg_head.predictor.bbox_embed.8.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,564] Loaded sem_seg_head.predictor.bbox_embed.8.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,564] Loaded sem_seg_head.predictor.bbox_embed.8.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,564] Loaded sem_seg_head.predictor.bbox_embed.8.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:03,564] Loaded sem_seg_head.predictor.bbox_embed.8.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:03,564] Loaded sem_seg_head.predictor.class_embed, Model Shape: torch.Size([256, 512]) <-> Ckpt Shape: torch.Size([256, 512])
[2023-06-26 17:34:03,564] Loaded sem_seg_head.predictor.decoder.bbox_embed.0.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,564] Loaded sem_seg_head.predictor.decoder.bbox_embed.0.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,564] Loaded sem_seg_head.predictor.decoder.bbox_embed.0.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,564] Loaded sem_seg_head.predictor.decoder.bbox_embed.0.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,564] Loaded sem_seg_head.predictor.decoder.bbox_embed.0.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:03,564] Loaded sem_seg_head.predictor.decoder.bbox_embed.0.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:03,564] Loaded sem_seg_head.predictor.decoder.bbox_embed.1.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,564] Loaded sem_seg_head.predictor.decoder.bbox_embed.1.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,564] Loaded sem_seg_head.predictor.decoder.bbox_embed.1.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,564] Loaded sem_seg_head.predictor.decoder.bbox_embed.1.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,565] Loaded sem_seg_head.predictor.decoder.bbox_embed.1.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:03,565] Loaded sem_seg_head.predictor.decoder.bbox_embed.1.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:03,565] Loaded sem_seg_head.predictor.decoder.bbox_embed.2.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,565] Loaded sem_seg_head.predictor.decoder.bbox_embed.2.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,565] Loaded sem_seg_head.predictor.decoder.bbox_embed.2.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,565] Loaded sem_seg_head.predictor.decoder.bbox_embed.2.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,565] Loaded sem_seg_head.predictor.decoder.bbox_embed.2.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:03,565] Loaded sem_seg_head.predictor.decoder.bbox_embed.2.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:03,565] Loaded sem_seg_head.predictor.decoder.bbox_embed.3.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,565] Loaded sem_seg_head.predictor.decoder.bbox_embed.3.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,565] Loaded sem_seg_head.predictor.decoder.bbox_embed.3.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,565] Loaded sem_seg_head.predictor.decoder.bbox_embed.3.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,565] Loaded sem_seg_head.predictor.decoder.bbox_embed.3.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:03,565] Loaded sem_seg_head.predictor.decoder.bbox_embed.3.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:03,565] Loaded sem_seg_head.predictor.decoder.bbox_embed.4.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,565] Loaded sem_seg_head.predictor.decoder.bbox_embed.4.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,565] Loaded sem_seg_head.predictor.decoder.bbox_embed.4.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,565] Loaded sem_seg_head.predictor.decoder.bbox_embed.4.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,566] Loaded sem_seg_head.predictor.decoder.bbox_embed.4.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:03,566] Loaded sem_seg_head.predictor.decoder.bbox_embed.4.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:03,566] Loaded sem_seg_head.predictor.decoder.bbox_embed.5.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,566] Loaded sem_seg_head.predictor.decoder.bbox_embed.5.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,566] Loaded sem_seg_head.predictor.decoder.bbox_embed.5.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,566] Loaded sem_seg_head.predictor.decoder.bbox_embed.5.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,566] Loaded sem_seg_head.predictor.decoder.bbox_embed.5.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:03,566] Loaded sem_seg_head.predictor.decoder.bbox_embed.5.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:03,566] Loaded sem_seg_head.predictor.decoder.bbox_embed.6.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,566] Loaded sem_seg_head.predictor.decoder.bbox_embed.6.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,566] Loaded sem_seg_head.predictor.decoder.bbox_embed.6.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,566] Loaded sem_seg_head.predictor.decoder.bbox_embed.6.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,566] Loaded sem_seg_head.predictor.decoder.bbox_embed.6.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:03,566] Loaded sem_seg_head.predictor.decoder.bbox_embed.6.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:03,566] Loaded sem_seg_head.predictor.decoder.bbox_embed.7.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,566] Loaded sem_seg_head.predictor.decoder.bbox_embed.7.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,566] Loaded sem_seg_head.predictor.decoder.bbox_embed.7.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,566] Loaded sem_seg_head.predictor.decoder.bbox_embed.7.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,566] Loaded sem_seg_head.predictor.decoder.bbox_embed.7.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:03,567] Loaded sem_seg_head.predictor.decoder.bbox_embed.7.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:03,567] Loaded sem_seg_head.predictor.decoder.bbox_embed.8.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,567] Loaded sem_seg_head.predictor.decoder.bbox_embed.8.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,567] Loaded sem_seg_head.predictor.decoder.bbox_embed.8.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,567] Loaded sem_seg_head.predictor.decoder.bbox_embed.8.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,567] Loaded sem_seg_head.predictor.decoder.bbox_embed.8.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:03,567] Loaded sem_seg_head.predictor.decoder.bbox_embed.8.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:03,567] Loaded sem_seg_head.predictor.decoder.layers.0.cross_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:03,567] Loaded sem_seg_head.predictor.decoder.layers.0.cross_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:03,567] Loaded sem_seg_head.predictor.decoder.layers.0.cross_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,567] Loaded sem_seg_head.predictor.decoder.layers.0.cross_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,567] Loaded sem_seg_head.predictor.decoder.layers.0.cross_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,567] Loaded sem_seg_head.predictor.decoder.layers.0.cross_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,567] Loaded sem_seg_head.predictor.decoder.layers.0.cross_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,567] Loaded sem_seg_head.predictor.decoder.layers.0.cross_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,567] Loaded sem_seg_head.predictor.decoder.layers.0.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,567] Loaded sem_seg_head.predictor.decoder.layers.0.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:03,567] Loaded sem_seg_head.predictor.decoder.layers.0.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,568] Loaded sem_seg_head.predictor.decoder.layers.0.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:03,568] Loaded sem_seg_head.predictor.decoder.layers.0.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,568] Loaded sem_seg_head.predictor.decoder.layers.0.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,568] Loaded sem_seg_head.predictor.decoder.layers.0.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,568] Loaded sem_seg_head.predictor.decoder.layers.0.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,568] Loaded sem_seg_head.predictor.decoder.layers.0.norm3.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,568] Loaded sem_seg_head.predictor.decoder.layers.0.norm3.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,568] Loaded sem_seg_head.predictor.decoder.layers.0.self_attn.in_proj_bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,568] Loaded sem_seg_head.predictor.decoder.layers.0.self_attn.in_proj_weight, Model Shape: torch.Size([768, 256]) <-> Ckpt Shape: torch.Size([768, 256])
[2023-06-26 17:34:03,568] Loaded sem_seg_head.predictor.decoder.layers.0.self_attn.out_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,568] Loaded sem_seg_head.predictor.decoder.layers.0.self_attn.out_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,568] Loaded sem_seg_head.predictor.decoder.layers.1.cross_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:03,568] Loaded sem_seg_head.predictor.decoder.layers.1.cross_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:03,568] Loaded sem_seg_head.predictor.decoder.layers.1.cross_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,568] Loaded sem_seg_head.predictor.decoder.layers.1.cross_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,568] Loaded sem_seg_head.predictor.decoder.layers.1.cross_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,568] Loaded sem_seg_head.predictor.decoder.layers.1.cross_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,569] Loaded sem_seg_head.predictor.decoder.layers.1.cross_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,569] Loaded sem_seg_head.predictor.decoder.layers.1.cross_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,569] Loaded sem_seg_head.predictor.decoder.layers.1.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,569] Loaded sem_seg_head.predictor.decoder.layers.1.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:03,569] Loaded sem_seg_head.predictor.decoder.layers.1.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,569] Loaded sem_seg_head.predictor.decoder.layers.1.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:03,569] Loaded sem_seg_head.predictor.decoder.layers.1.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,569] Loaded sem_seg_head.predictor.decoder.layers.1.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,569] Loaded sem_seg_head.predictor.decoder.layers.1.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,569] Loaded sem_seg_head.predictor.decoder.layers.1.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,569] Loaded sem_seg_head.predictor.decoder.layers.1.norm3.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,569] Loaded sem_seg_head.predictor.decoder.layers.1.norm3.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,569] Loaded sem_seg_head.predictor.decoder.layers.1.self_attn.in_proj_bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,569] Loaded sem_seg_head.predictor.decoder.layers.1.self_attn.in_proj_weight, Model Shape: torch.Size([768, 256]) <-> Ckpt Shape: torch.Size([768, 256])
[2023-06-26 17:34:03,569] Loaded sem_seg_head.predictor.decoder.layers.1.self_attn.out_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,569] Loaded sem_seg_head.predictor.decoder.layers.1.self_attn.out_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,569] Loaded sem_seg_head.predictor.decoder.layers.2.cross_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:03,570] Loaded sem_seg_head.predictor.decoder.layers.2.cross_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:03,570] Loaded sem_seg_head.predictor.decoder.layers.2.cross_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,570] Loaded sem_seg_head.predictor.decoder.layers.2.cross_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,570] Loaded sem_seg_head.predictor.decoder.layers.2.cross_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,570] Loaded sem_seg_head.predictor.decoder.layers.2.cross_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,570] Loaded sem_seg_head.predictor.decoder.layers.2.cross_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,570] Loaded sem_seg_head.predictor.decoder.layers.2.cross_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,570] Loaded sem_seg_head.predictor.decoder.layers.2.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,570] Loaded sem_seg_head.predictor.decoder.layers.2.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:03,570] Loaded sem_seg_head.predictor.decoder.layers.2.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,570] Loaded sem_seg_head.predictor.decoder.layers.2.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:03,570] Loaded sem_seg_head.predictor.decoder.layers.2.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,570] Loaded sem_seg_head.predictor.decoder.layers.2.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,570] Loaded sem_seg_head.predictor.decoder.layers.2.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,570] Loaded sem_seg_head.predictor.decoder.layers.2.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,570] Loaded sem_seg_head.predictor.decoder.layers.2.norm3.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,570] Loaded sem_seg_head.predictor.decoder.layers.2.norm3.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,571] Loaded sem_seg_head.predictor.decoder.layers.2.self_attn.in_proj_bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,571] Loaded sem_seg_head.predictor.decoder.layers.2.self_attn.in_proj_weight, Model Shape: torch.Size([768, 256]) <-> Ckpt Shape: torch.Size([768, 256])
[2023-06-26 17:34:03,571] Loaded sem_seg_head.predictor.decoder.layers.2.self_attn.out_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,571] Loaded sem_seg_head.predictor.decoder.layers.2.self_attn.out_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,571] Loaded sem_seg_head.predictor.decoder.layers.3.cross_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:03,571] Loaded sem_seg_head.predictor.decoder.layers.3.cross_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:03,571] Loaded sem_seg_head.predictor.decoder.layers.3.cross_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,571] Loaded sem_seg_head.predictor.decoder.layers.3.cross_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,571] Loaded sem_seg_head.predictor.decoder.layers.3.cross_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,571] Loaded sem_seg_head.predictor.decoder.layers.3.cross_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,571] Loaded sem_seg_head.predictor.decoder.layers.3.cross_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,571] Loaded sem_seg_head.predictor.decoder.layers.3.cross_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,571] Loaded sem_seg_head.predictor.decoder.layers.3.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,571] Loaded sem_seg_head.predictor.decoder.layers.3.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:03,571] Loaded sem_seg_head.predictor.decoder.layers.3.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,571] Loaded sem_seg_head.predictor.decoder.layers.3.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:03,571] Loaded sem_seg_head.predictor.decoder.layers.3.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,572] Loaded sem_seg_head.predictor.decoder.layers.3.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,572] Loaded sem_seg_head.predictor.decoder.layers.3.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,572] Loaded sem_seg_head.predictor.decoder.layers.3.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,572] Loaded sem_seg_head.predictor.decoder.layers.3.norm3.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,572] Loaded sem_seg_head.predictor.decoder.layers.3.norm3.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,572] Loaded sem_seg_head.predictor.decoder.layers.3.self_attn.in_proj_bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,572] Loaded sem_seg_head.predictor.decoder.layers.3.self_attn.in_proj_weight, Model Shape: torch.Size([768, 256]) <-> Ckpt Shape: torch.Size([768, 256])
[2023-06-26 17:34:03,572] Loaded sem_seg_head.predictor.decoder.layers.3.self_attn.out_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,572] Loaded sem_seg_head.predictor.decoder.layers.3.self_attn.out_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,572] Loaded sem_seg_head.predictor.decoder.layers.4.cross_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:03,572] Loaded sem_seg_head.predictor.decoder.layers.4.cross_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:03,572] Loaded sem_seg_head.predictor.decoder.layers.4.cross_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,572] Loaded sem_seg_head.predictor.decoder.layers.4.cross_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,572] Loaded sem_seg_head.predictor.decoder.layers.4.cross_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,572] Loaded sem_seg_head.predictor.decoder.layers.4.cross_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,573] Loaded sem_seg_head.predictor.decoder.layers.4.cross_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,573] Loaded sem_seg_head.predictor.decoder.layers.4.cross_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,573] Loaded sem_seg_head.predictor.decoder.layers.4.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,573] Loaded sem_seg_head.predictor.decoder.layers.4.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:03,573] Loaded sem_seg_head.predictor.decoder.layers.4.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,573] Loaded sem_seg_head.predictor.decoder.layers.4.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:03,573] Loaded sem_seg_head.predictor.decoder.layers.4.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,573] Loaded sem_seg_head.predictor.decoder.layers.4.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,573] Loaded sem_seg_head.predictor.decoder.layers.4.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,573] Loaded sem_seg_head.predictor.decoder.layers.4.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,573] Loaded sem_seg_head.predictor.decoder.layers.4.norm3.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,573] Loaded sem_seg_head.predictor.decoder.layers.4.norm3.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,573] Loaded sem_seg_head.predictor.decoder.layers.4.self_attn.in_proj_bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,573] Loaded sem_seg_head.predictor.decoder.layers.4.self_attn.in_proj_weight, Model Shape: torch.Size([768, 256]) <-> Ckpt Shape: torch.Size([768, 256])
[2023-06-26 17:34:03,573] Loaded sem_seg_head.predictor.decoder.layers.4.self_attn.out_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,573] Loaded sem_seg_head.predictor.decoder.layers.4.self_attn.out_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,573] Loaded sem_seg_head.predictor.decoder.layers.5.cross_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:03,574] Loaded sem_seg_head.predictor.decoder.layers.5.cross_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:03,574] Loaded sem_seg_head.predictor.decoder.layers.5.cross_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,574] Loaded sem_seg_head.predictor.decoder.layers.5.cross_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,574] Loaded sem_seg_head.predictor.decoder.layers.5.cross_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,574] Loaded sem_seg_head.predictor.decoder.layers.5.cross_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,574] Loaded sem_seg_head.predictor.decoder.layers.5.cross_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,574] Loaded sem_seg_head.predictor.decoder.layers.5.cross_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,574] Loaded sem_seg_head.predictor.decoder.layers.5.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,574] Loaded sem_seg_head.predictor.decoder.layers.5.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:03,574] Loaded sem_seg_head.predictor.decoder.layers.5.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,574] Loaded sem_seg_head.predictor.decoder.layers.5.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:03,574] Loaded sem_seg_head.predictor.decoder.layers.5.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,574] Loaded sem_seg_head.predictor.decoder.layers.5.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,574] Loaded sem_seg_head.predictor.decoder.layers.5.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,574] Loaded sem_seg_head.predictor.decoder.layers.5.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,574] Loaded sem_seg_head.predictor.decoder.layers.5.norm3.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,574] Loaded sem_seg_head.predictor.decoder.layers.5.norm3.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,574] Loaded sem_seg_head.predictor.decoder.layers.5.self_attn.in_proj_bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,575] Loaded sem_seg_head.predictor.decoder.layers.5.self_attn.in_proj_weight, Model Shape: torch.Size([768, 256]) <-> Ckpt Shape: torch.Size([768, 256])
[2023-06-26 17:34:03,575] Loaded sem_seg_head.predictor.decoder.layers.5.self_attn.out_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,575] Loaded sem_seg_head.predictor.decoder.layers.5.self_attn.out_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,575] Loaded sem_seg_head.predictor.decoder.layers.6.cross_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:03,575] Loaded sem_seg_head.predictor.decoder.layers.6.cross_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:03,576] Loaded sem_seg_head.predictor.decoder.layers.6.cross_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,576] Loaded sem_seg_head.predictor.decoder.layers.6.cross_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,576] Loaded sem_seg_head.predictor.decoder.layers.6.cross_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,576] Loaded sem_seg_head.predictor.decoder.layers.6.cross_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,576] Loaded sem_seg_head.predictor.decoder.layers.6.cross_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,576] Loaded sem_seg_head.predictor.decoder.layers.6.cross_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,576] Loaded sem_seg_head.predictor.decoder.layers.6.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,576] Loaded sem_seg_head.predictor.decoder.layers.6.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:03,576] Loaded sem_seg_head.predictor.decoder.layers.6.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,576] Loaded sem_seg_head.predictor.decoder.layers.6.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:03,576] Loaded sem_seg_head.predictor.decoder.layers.6.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,576] Loaded sem_seg_head.predictor.decoder.layers.6.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,576] Loaded sem_seg_head.predictor.decoder.layers.6.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,576] Loaded sem_seg_head.predictor.decoder.layers.6.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,576] Loaded sem_seg_head.predictor.decoder.layers.6.norm3.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,576] Loaded sem_seg_head.predictor.decoder.layers.6.norm3.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,576] Loaded sem_seg_head.predictor.decoder.layers.6.self_attn.in_proj_bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,577] Loaded sem_seg_head.predictor.decoder.layers.6.self_attn.in_proj_weight, Model Shape: torch.Size([768, 256]) <-> Ckpt Shape: torch.Size([768, 256])
[2023-06-26 17:34:03,577] Loaded sem_seg_head.predictor.decoder.layers.6.self_attn.out_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,577] Loaded sem_seg_head.predictor.decoder.layers.6.self_attn.out_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,577] Loaded sem_seg_head.predictor.decoder.layers.7.cross_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:03,577] Loaded sem_seg_head.predictor.decoder.layers.7.cross_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:03,577] Loaded sem_seg_head.predictor.decoder.layers.7.cross_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,577] Loaded sem_seg_head.predictor.decoder.layers.7.cross_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,577] Loaded sem_seg_head.predictor.decoder.layers.7.cross_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,577] Loaded sem_seg_head.predictor.decoder.layers.7.cross_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,577] Loaded sem_seg_head.predictor.decoder.layers.7.cross_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,577] Loaded sem_seg_head.predictor.decoder.layers.7.cross_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,577] Loaded sem_seg_head.predictor.decoder.layers.7.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,577] Loaded sem_seg_head.predictor.decoder.layers.7.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:03,577] Loaded sem_seg_head.predictor.decoder.layers.7.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,577] Loaded sem_seg_head.predictor.decoder.layers.7.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:03,577] Loaded sem_seg_head.predictor.decoder.layers.7.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,577] Loaded sem_seg_head.predictor.decoder.layers.7.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,577] Loaded sem_seg_head.predictor.decoder.layers.7.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,578] Loaded sem_seg_head.predictor.decoder.layers.7.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,578] Loaded sem_seg_head.predictor.decoder.layers.7.norm3.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,578] Loaded sem_seg_head.predictor.decoder.layers.7.norm3.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,578] Loaded sem_seg_head.predictor.decoder.layers.7.self_attn.in_proj_bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,578] Loaded sem_seg_head.predictor.decoder.layers.7.self_attn.in_proj_weight, Model Shape: torch.Size([768, 256]) <-> Ckpt Shape: torch.Size([768, 256])
[2023-06-26 17:34:03,578] Loaded sem_seg_head.predictor.decoder.layers.7.self_attn.out_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,578] Loaded sem_seg_head.predictor.decoder.layers.7.self_attn.out_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,578] Loaded sem_seg_head.predictor.decoder.layers.8.cross_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:03,578] Loaded sem_seg_head.predictor.decoder.layers.8.cross_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:03,579] Loaded sem_seg_head.predictor.decoder.layers.8.cross_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,579] Loaded sem_seg_head.predictor.decoder.layers.8.cross_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,579] Loaded sem_seg_head.predictor.decoder.layers.8.cross_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,579] Loaded sem_seg_head.predictor.decoder.layers.8.cross_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,579] Loaded sem_seg_head.predictor.decoder.layers.8.cross_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,579] Loaded sem_seg_head.predictor.decoder.layers.8.cross_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,579] Loaded sem_seg_head.predictor.decoder.layers.8.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,579] Loaded sem_seg_head.predictor.decoder.layers.8.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:03,579] Loaded sem_seg_head.predictor.decoder.layers.8.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,579] Loaded sem_seg_head.predictor.decoder.layers.8.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:03,579] Loaded sem_seg_head.predictor.decoder.layers.8.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,579] Loaded sem_seg_head.predictor.decoder.layers.8.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,580] Loaded sem_seg_head.predictor.decoder.layers.8.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,580] Loaded sem_seg_head.predictor.decoder.layers.8.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,580] Loaded sem_seg_head.predictor.decoder.layers.8.norm3.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,580] Loaded sem_seg_head.predictor.decoder.layers.8.norm3.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,580] Loaded sem_seg_head.predictor.decoder.layers.8.self_attn.in_proj_bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:03,580] Loaded sem_seg_head.predictor.decoder.layers.8.self_attn.in_proj_weight, Model Shape: torch.Size([768, 256]) <-> Ckpt Shape: torch.Size([768, 256])
[2023-06-26 17:34:03,580] Loaded sem_seg_head.predictor.decoder.layers.8.self_attn.out_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,580] Loaded sem_seg_head.predictor.decoder.layers.8.self_attn.out_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,580] Loaded sem_seg_head.predictor.decoder.norm.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,580] Loaded sem_seg_head.predictor.decoder.norm.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,580] Loaded sem_seg_head.predictor.decoder.ref_point_head.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,580] Loaded sem_seg_head.predictor.decoder.ref_point_head.layers.0.weight, Model Shape: torch.Size([256, 512]) <-> Ckpt Shape: torch.Size([256, 512])
[2023-06-26 17:34:03,581] Loaded sem_seg_head.predictor.decoder.ref_point_head.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,581] Loaded sem_seg_head.predictor.decoder.ref_point_head.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,581] Loaded sem_seg_head.predictor.decoder_norm.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,581] Loaded sem_seg_head.predictor.decoder_norm.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,581] Loaded sem_seg_head.predictor.enc_output.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,582] Loaded sem_seg_head.predictor.enc_output.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,582] Loaded sem_seg_head.predictor.enc_output_norm.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,582] Loaded sem_seg_head.predictor.enc_output_norm.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.ln_final.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.ln_final.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:03,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])
[2023-06-26 17:34:03,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:03,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])
[2023-06-26 17:34:03,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])
[2023-06-26 17:34:03,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:03,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])
[2023-06-26 17:34:03,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:03,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])
[2023-06-26 17:34:03,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])
[2023-06-26 17:34:03,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:03,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])
[2023-06-26 17:34:03,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:03,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])
[2023-06-26 17:34:03,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])
[2023-06-26 17:34:03,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:03,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])
[2023-06-26 17:34:03,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:03,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])
[2023-06-26 17:34:03,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])
[2023-06-26 17:34:03,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:03,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])
[2023-06-26 17:34:03,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:03,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,585] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,585] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,585] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,585] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,585] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])
[2023-06-26 17:34:03,585] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,585] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])
[2023-06-26 17:34:03,585] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:03,585] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])
[2023-06-26 17:34:03,585] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,585] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:03,585] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,585] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,585] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,585] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,585] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,585] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])
[2023-06-26 17:34:03,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])
[2023-06-26 17:34:03,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:03,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])
[2023-06-26 17:34:03,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:03,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])
[2023-06-26 17:34:03,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])
[2023-06-26 17:34:03,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:03,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])
[2023-06-26 17:34:03,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:03,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])
[2023-06-26 17:34:03,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])
[2023-06-26 17:34:03,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:03,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])
[2023-06-26 17:34:03,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:03,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])
[2023-06-26 17:34:03,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])
[2023-06-26 17:34:03,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:03,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])
[2023-06-26 17:34:03,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:03,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])
[2023-06-26 17:34:03,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])
[2023-06-26 17:34:03,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:03,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])
[2023-06-26 17:34:03,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:03,589] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,589] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,589] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,589] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,589] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,589] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])
[2023-06-26 17:34:03,589] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,589] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])
[2023-06-26 17:34:03,589] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:03,589] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])
[2023-06-26 17:34:03,589] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,589] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:03,589] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,589] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,590] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,590] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,590] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:03,590] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])
[2023-06-26 17:34:03,590] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:03,590] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])
[2023-06-26 17:34:03,590] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.token_embedding.weight, Model Shape: torch.Size([49408, 512]) <-> Ckpt Shape: torch.Size([49408, 512])
[2023-06-26 17:34:03,590] Loaded sem_seg_head.predictor.lang_encoder.lang_proj, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:03,590] Loaded sem_seg_head.predictor.lang_encoder.logit_scale, Model Shape: torch.Size([]) <-> Ckpt Shape: torch.Size([])
[2023-06-26 17:34:03,590] Loaded sem_seg_head.predictor.lang_mapper, Model Shape: torch.Size([512, 256]) <-> Ckpt Shape: torch.Size([512, 256])
[2023-06-26 17:34:03,590] Loaded sem_seg_head.predictor.mask_embed.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,590] Loaded sem_seg_head.predictor.mask_embed.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,590] Loaded sem_seg_head.predictor.mask_embed.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,590] Loaded sem_seg_head.predictor.mask_embed.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,590] Loaded sem_seg_head.predictor.mask_embed.layers.2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:03,590] Loaded sem_seg_head.predictor.mask_embed.layers.2.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:03,590] $UNUSED$ criterion.empty_weight, Ckpt Shape: torch.Size([134])
[2023-06-26 17:34:03,591] $UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.positional_embedding, Ckpt Shape: torch.Size([18, 512])
[2023-06-26 17:34:03,591] *UNMATCHED* sem_seg_head.predictor.lang_encoder.lang_encoder.positional_embedding, Model Shape: torch.Size([77, 512]) <-> Ckpt Shape: torch.Size([18, 512])
[2023-06-26 17:34:34,510] DATA:
  IMGS_DIR: /srv/datasets2/nutrition5k_dataset/imagery/realsense_overhead
  METADATAS_PATH: /srv/datasets2/nutrition5k_dataset/metadata/dish_metadata_cafe1.csv
  NAME: nutrition5k
  SPLITS_TEST_PATH: /srv/datasets2/nutrition5k_dataset/dish_ids/splits/depth_test_ids.txt
  SPLITS_TRAIN_PATH: /srv/datasets2/nutrition5k_dataset/dish_ids/splits/depth_train_ids.txt
EVAL:
  HEIGHT: 384
  WIDTH: 512
MODEL:
  MASK_WEIGHT: 0.5
  NAME: openseed
  PRETRAINED: ''
SAVE_PATH: models/fixed/fpnopenseed_wrot_fc3_nodrop_attninit_warmup_150ep_mltscl.pt
TITLE:
- fpn openseed with aug+rotate fc 3 no dropout freeze new attn initialize with warmup
  150ep multi scale
TRAIN:
  BATCH_SIZE: 32
  CKPT: null
  FINETUNE: false
  LAYERS: 1
  LOSS: multi
  LR: 5.0e-05
  NUM_EPOCHS: 150
  SEED: 12345
  WEIGHT_DECAY: 0.0001

[2023-06-26 17:34:35,318] URL https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth cached in /home/parinayok/.torch/iopath_cache/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth
[2023-06-26 17:34:37,683] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,686] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,686] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,693] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,694] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,700] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,700] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,701] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,701] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,707] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,707] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,714] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,714] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,715] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,715] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,722] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,722] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,728] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,728] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,730] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,730] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,736] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,736] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,742] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,742] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,744] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,744] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,750] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,750] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,756] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,756] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,758] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,758] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,764] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,764] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,770] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,770] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,772] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,772] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,778] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,778] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,784] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,784] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,785] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,786] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,792] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,792] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,798] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,798] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,799] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,799] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,805] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,806] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,812] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,812] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,813] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,813] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,819] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,819] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,825] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,825] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,827] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,827] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,833] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,833] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,839] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,839] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,841] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,841] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,847] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:37,847] => init weight of Linear/Conv2d from trunc norm
[2023-06-26 17:34:37,853] => init bias of Linear/Conv2d to zeros
[2023-06-26 17:34:38,544] Loaded backbone.layers.0.blocks.0.attn.proj.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:38,544] Loaded backbone.layers.0.blocks.0.attn.proj.weight, Model Shape: torch.Size([96, 96]) <-> Ckpt Shape: torch.Size([96, 96])
[2023-06-26 17:34:38,544] Loaded backbone.layers.0.blocks.0.attn.qkv.bias, Model Shape: torch.Size([288]) <-> Ckpt Shape: torch.Size([288])
[2023-06-26 17:34:38,544] Loaded backbone.layers.0.blocks.0.attn.qkv.weight, Model Shape: torch.Size([288, 96]) <-> Ckpt Shape: torch.Size([288, 96])
[2023-06-26 17:34:38,544] Loaded backbone.layers.0.blocks.0.attn.relative_position_bias_table, Model Shape: torch.Size([169, 3]) <-> Ckpt Shape: torch.Size([169, 3])
[2023-06-26 17:34:38,544] Loaded backbone.layers.0.blocks.0.attn.relative_position_index, Model Shape: torch.Size([49, 49]) <-> Ckpt Shape: torch.Size([49, 49])
[2023-06-26 17:34:38,544] Loaded backbone.layers.0.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,544] Loaded backbone.layers.0.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([384, 96]) <-> Ckpt Shape: torch.Size([384, 96])
[2023-06-26 17:34:38,544] Loaded backbone.layers.0.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:38,544] Loaded backbone.layers.0.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([96, 384]) <-> Ckpt Shape: torch.Size([96, 384])
[2023-06-26 17:34:38,544] Loaded backbone.layers.0.blocks.0.norm1.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:38,544] Loaded backbone.layers.0.blocks.0.norm1.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:38,544] Loaded backbone.layers.0.blocks.0.norm2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:38,544] Loaded backbone.layers.0.blocks.0.norm2.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:38,545] Loaded backbone.layers.0.blocks.1.attn.proj.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:38,545] Loaded backbone.layers.0.blocks.1.attn.proj.weight, Model Shape: torch.Size([96, 96]) <-> Ckpt Shape: torch.Size([96, 96])
[2023-06-26 17:34:38,545] Loaded backbone.layers.0.blocks.1.attn.qkv.bias, Model Shape: torch.Size([288]) <-> Ckpt Shape: torch.Size([288])
[2023-06-26 17:34:38,545] Loaded backbone.layers.0.blocks.1.attn.qkv.weight, Model Shape: torch.Size([288, 96]) <-> Ckpt Shape: torch.Size([288, 96])
[2023-06-26 17:34:38,545] Loaded backbone.layers.0.blocks.1.attn.relative_position_bias_table, Model Shape: torch.Size([169, 3]) <-> Ckpt Shape: torch.Size([169, 3])
[2023-06-26 17:34:38,545] Loaded backbone.layers.0.blocks.1.attn.relative_position_index, Model Shape: torch.Size([49, 49]) <-> Ckpt Shape: torch.Size([49, 49])
[2023-06-26 17:34:38,545] Loaded backbone.layers.0.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,545] Loaded backbone.layers.0.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([384, 96]) <-> Ckpt Shape: torch.Size([384, 96])
[2023-06-26 17:34:38,545] Loaded backbone.layers.0.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:38,545] Loaded backbone.layers.0.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([96, 384]) <-> Ckpt Shape: torch.Size([96, 384])
[2023-06-26 17:34:38,545] Loaded backbone.layers.0.blocks.1.norm1.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:38,545] Loaded backbone.layers.0.blocks.1.norm1.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:38,545] Loaded backbone.layers.0.blocks.1.norm2.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:38,545] Loaded backbone.layers.0.blocks.1.norm2.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:38,545] Loaded backbone.layers.0.downsample.norm.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,545] Loaded backbone.layers.0.downsample.norm.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,545] Loaded backbone.layers.0.downsample.reduction.weight, Model Shape: torch.Size([192, 384]) <-> Ckpt Shape: torch.Size([192, 384])
[2023-06-26 17:34:38,545] Loaded backbone.layers.1.blocks.0.attn.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:38,545] Loaded backbone.layers.1.blocks.0.attn.proj.weight, Model Shape: torch.Size([192, 192]) <-> Ckpt Shape: torch.Size([192, 192])
[2023-06-26 17:34:38,545] Loaded backbone.layers.1.blocks.0.attn.qkv.bias, Model Shape: torch.Size([576]) <-> Ckpt Shape: torch.Size([576])
[2023-06-26 17:34:38,545] Loaded backbone.layers.1.blocks.0.attn.qkv.weight, Model Shape: torch.Size([576, 192]) <-> Ckpt Shape: torch.Size([576, 192])
[2023-06-26 17:34:38,545] Loaded backbone.layers.1.blocks.0.attn.relative_position_bias_table, Model Shape: torch.Size([169, 6]) <-> Ckpt Shape: torch.Size([169, 6])
[2023-06-26 17:34:38,545] Loaded backbone.layers.1.blocks.0.attn.relative_position_index, Model Shape: torch.Size([49, 49]) <-> Ckpt Shape: torch.Size([49, 49])
[2023-06-26 17:34:38,545] Loaded backbone.layers.1.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,545] Loaded backbone.layers.1.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])
[2023-06-26 17:34:38,545] Loaded backbone.layers.1.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:38,545] Loaded backbone.layers.1.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])
[2023-06-26 17:34:38,545] Loaded backbone.layers.1.blocks.0.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:38,545] Loaded backbone.layers.1.blocks.0.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:38,546] Loaded backbone.layers.1.blocks.0.norm2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:38,546] Loaded backbone.layers.1.blocks.0.norm2.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:38,546] Loaded backbone.layers.1.blocks.1.attn.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:38,546] Loaded backbone.layers.1.blocks.1.attn.proj.weight, Model Shape: torch.Size([192, 192]) <-> Ckpt Shape: torch.Size([192, 192])
[2023-06-26 17:34:38,546] Loaded backbone.layers.1.blocks.1.attn.qkv.bias, Model Shape: torch.Size([576]) <-> Ckpt Shape: torch.Size([576])
[2023-06-26 17:34:38,546] Loaded backbone.layers.1.blocks.1.attn.qkv.weight, Model Shape: torch.Size([576, 192]) <-> Ckpt Shape: torch.Size([576, 192])
[2023-06-26 17:34:38,546] Loaded backbone.layers.1.blocks.1.attn.relative_position_bias_table, Model Shape: torch.Size([169, 6]) <-> Ckpt Shape: torch.Size([169, 6])
[2023-06-26 17:34:38,546] Loaded backbone.layers.1.blocks.1.attn.relative_position_index, Model Shape: torch.Size([49, 49]) <-> Ckpt Shape: torch.Size([49, 49])
[2023-06-26 17:34:38,546] Loaded backbone.layers.1.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,546] Loaded backbone.layers.1.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])
[2023-06-26 17:34:38,546] Loaded backbone.layers.1.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:38,546] Loaded backbone.layers.1.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])
[2023-06-26 17:34:38,546] Loaded backbone.layers.1.blocks.1.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:38,546] Loaded backbone.layers.1.blocks.1.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:38,546] Loaded backbone.layers.1.blocks.1.norm2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:38,546] Loaded backbone.layers.1.blocks.1.norm2.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:38,546] Loaded backbone.layers.1.downsample.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,546] Loaded backbone.layers.1.downsample.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,546] Loaded backbone.layers.1.downsample.reduction.weight, Model Shape: torch.Size([384, 768]) <-> Ckpt Shape: torch.Size([384, 768])
[2023-06-26 17:34:38,546] Loaded backbone.layers.2.blocks.0.attn.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,546] Loaded backbone.layers.2.blocks.0.attn.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])
[2023-06-26 17:34:38,546] Loaded backbone.layers.2.blocks.0.attn.qkv.bias, Model Shape: torch.Size([1152]) <-> Ckpt Shape: torch.Size([1152])
[2023-06-26 17:34:38,546] Loaded backbone.layers.2.blocks.0.attn.qkv.weight, Model Shape: torch.Size([1152, 384]) <-> Ckpt Shape: torch.Size([1152, 384])
[2023-06-26 17:34:38,546] Loaded backbone.layers.2.blocks.0.attn.relative_position_bias_table, Model Shape: torch.Size([169, 12]) <-> Ckpt Shape: torch.Size([169, 12])
[2023-06-26 17:34:38,546] Loaded backbone.layers.2.blocks.0.attn.relative_position_index, Model Shape: torch.Size([49, 49]) <-> Ckpt Shape: torch.Size([49, 49])
[2023-06-26 17:34:38,546] Loaded backbone.layers.2.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:38,546] Loaded backbone.layers.2.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.0.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.0.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.0.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.0.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.1.attn.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.1.attn.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.1.attn.qkv.bias, Model Shape: torch.Size([1152]) <-> Ckpt Shape: torch.Size([1152])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.1.attn.qkv.weight, Model Shape: torch.Size([1152, 384]) <-> Ckpt Shape: torch.Size([1152, 384])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.1.attn.relative_position_bias_table, Model Shape: torch.Size([169, 12]) <-> Ckpt Shape: torch.Size([169, 12])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.1.attn.relative_position_index, Model Shape: torch.Size([49, 49]) <-> Ckpt Shape: torch.Size([49, 49])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.1.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.1.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.1.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.1.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.2.attn.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.2.attn.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.2.attn.qkv.bias, Model Shape: torch.Size([1152]) <-> Ckpt Shape: torch.Size([1152])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.2.attn.qkv.weight, Model Shape: torch.Size([1152, 384]) <-> Ckpt Shape: torch.Size([1152, 384])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.2.attn.relative_position_bias_table, Model Shape: torch.Size([169, 12]) <-> Ckpt Shape: torch.Size([169, 12])
[2023-06-26 17:34:38,547] Loaded backbone.layers.2.blocks.2.attn.relative_position_index, Model Shape: torch.Size([49, 49]) <-> Ckpt Shape: torch.Size([49, 49])
[2023-06-26 17:34:38,548] Loaded backbone.layers.2.blocks.2.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:38,548] Loaded backbone.layers.2.blocks.2.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])
[2023-06-26 17:34:38,548] Loaded backbone.layers.2.blocks.2.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,548] Loaded backbone.layers.2.blocks.2.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])
[2023-06-26 17:34:38,548] Loaded backbone.layers.2.blocks.2.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,548] Loaded backbone.layers.2.blocks.2.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,548] Loaded backbone.layers.2.blocks.2.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,548] Loaded backbone.layers.2.blocks.2.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,548] Loaded backbone.layers.2.blocks.3.attn.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,548] Loaded backbone.layers.2.blocks.3.attn.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])
[2023-06-26 17:34:38,548] Loaded backbone.layers.2.blocks.3.attn.qkv.bias, Model Shape: torch.Size([1152]) <-> Ckpt Shape: torch.Size([1152])
[2023-06-26 17:34:38,548] Loaded backbone.layers.2.blocks.3.attn.qkv.weight, Model Shape: torch.Size([1152, 384]) <-> Ckpt Shape: torch.Size([1152, 384])
[2023-06-26 17:34:38,548] Loaded backbone.layers.2.blocks.3.attn.relative_position_bias_table, Model Shape: torch.Size([169, 12]) <-> Ckpt Shape: torch.Size([169, 12])
[2023-06-26 17:34:38,548] Loaded backbone.layers.2.blocks.3.attn.relative_position_index, Model Shape: torch.Size([49, 49]) <-> Ckpt Shape: torch.Size([49, 49])
[2023-06-26 17:34:38,548] Loaded backbone.layers.2.blocks.3.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:38,548] Loaded backbone.layers.2.blocks.3.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])
[2023-06-26 17:34:38,548] Loaded backbone.layers.2.blocks.3.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,548] Loaded backbone.layers.2.blocks.3.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])
[2023-06-26 17:34:38,548] Loaded backbone.layers.2.blocks.3.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,548] Loaded backbone.layers.2.blocks.3.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,548] Loaded backbone.layers.2.blocks.3.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,548] Loaded backbone.layers.2.blocks.3.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.4.attn.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.4.attn.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.4.attn.qkv.bias, Model Shape: torch.Size([1152]) <-> Ckpt Shape: torch.Size([1152])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.4.attn.qkv.weight, Model Shape: torch.Size([1152, 384]) <-> Ckpt Shape: torch.Size([1152, 384])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.4.attn.relative_position_bias_table, Model Shape: torch.Size([169, 12]) <-> Ckpt Shape: torch.Size([169, 12])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.4.attn.relative_position_index, Model Shape: torch.Size([49, 49]) <-> Ckpt Shape: torch.Size([49, 49])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.4.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.4.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.4.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.4.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.4.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.4.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.4.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.4.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.5.attn.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.5.attn.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.5.attn.qkv.bias, Model Shape: torch.Size([1152]) <-> Ckpt Shape: torch.Size([1152])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.5.attn.qkv.weight, Model Shape: torch.Size([1152, 384]) <-> Ckpt Shape: torch.Size([1152, 384])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.5.attn.relative_position_bias_table, Model Shape: torch.Size([169, 12]) <-> Ckpt Shape: torch.Size([169, 12])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.5.attn.relative_position_index, Model Shape: torch.Size([49, 49]) <-> Ckpt Shape: torch.Size([49, 49])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.5.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.5.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.5.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.5.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.5.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.5.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.5.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.blocks.5.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.downsample.norm.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.downsample.norm.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:38,549] Loaded backbone.layers.2.downsample.reduction.weight, Model Shape: torch.Size([768, 1536]) <-> Ckpt Shape: torch.Size([768, 1536])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.0.attn.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.0.attn.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.0.attn.qkv.bias, Model Shape: torch.Size([2304]) <-> Ckpt Shape: torch.Size([2304])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.0.attn.qkv.weight, Model Shape: torch.Size([2304, 768]) <-> Ckpt Shape: torch.Size([2304, 768])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.0.attn.relative_position_bias_table, Model Shape: torch.Size([169, 24]) <-> Ckpt Shape: torch.Size([169, 24])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.0.attn.relative_position_index, Model Shape: torch.Size([49, 49]) <-> Ckpt Shape: torch.Size([49, 49])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.0.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.0.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.0.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.0.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.1.attn.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.1.attn.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.1.attn.qkv.bias, Model Shape: torch.Size([2304]) <-> Ckpt Shape: torch.Size([2304])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.1.attn.qkv.weight, Model Shape: torch.Size([2304, 768]) <-> Ckpt Shape: torch.Size([2304, 768])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.1.attn.relative_position_bias_table, Model Shape: torch.Size([169, 24]) <-> Ckpt Shape: torch.Size([169, 24])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.1.attn.relative_position_index, Model Shape: torch.Size([49, 49]) <-> Ckpt Shape: torch.Size([49, 49])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])
[2023-06-26 17:34:38,550] Loaded backbone.layers.3.blocks.1.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,551] Loaded backbone.layers.3.blocks.1.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,551] Loaded backbone.layers.3.blocks.1.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,551] Loaded backbone.layers.3.blocks.1.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,551] Loaded backbone.norm0.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:38,551] Loaded backbone.norm0.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:38,551] Loaded backbone.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:38,551] Loaded backbone.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])
[2023-06-26 17:34:38,551] Loaded backbone.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,551] Loaded backbone.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])
[2023-06-26 17:34:38,551] Loaded backbone.norm3.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,551] Loaded backbone.norm3.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,551] Loaded backbone.patch_embed.norm.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:38,551] Loaded backbone.patch_embed.norm.weight, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:38,551] Loaded backbone.patch_embed.proj.bias, Model Shape: torch.Size([96]) <-> Ckpt Shape: torch.Size([96])
[2023-06-26 17:34:38,551] Loaded backbone.patch_embed.proj.weight, Model Shape: torch.Size([96, 3, 4, 4]) <-> Ckpt Shape: torch.Size([96, 3, 4, 4])
[2023-06-26 17:34:38,551] Loaded sem_seg_head.pixel_decoder.adapter_1.norm.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,551] Loaded sem_seg_head.pixel_decoder.adapter_1.norm.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,551] Loaded sem_seg_head.pixel_decoder.adapter_1.weight, Model Shape: torch.Size([256, 96, 1, 1]) <-> Ckpt Shape: torch.Size([256, 96, 1, 1])
[2023-06-26 17:34:38,551] Loaded sem_seg_head.pixel_decoder.input_proj.0.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,551] Loaded sem_seg_head.pixel_decoder.input_proj.0.0.weight, Model Shape: torch.Size([256, 192, 1, 1]) <-> Ckpt Shape: torch.Size([256, 192, 1, 1])
[2023-06-26 17:34:38,551] Loaded sem_seg_head.pixel_decoder.input_proj.0.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,551] Loaded sem_seg_head.pixel_decoder.input_proj.0.1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,551] Loaded sem_seg_head.pixel_decoder.input_proj.1.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,551] Loaded sem_seg_head.pixel_decoder.input_proj.1.0.weight, Model Shape: torch.Size([256, 384, 1, 1]) <-> Ckpt Shape: torch.Size([256, 384, 1, 1])
[2023-06-26 17:34:38,551] Loaded sem_seg_head.pixel_decoder.input_proj.1.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,551] Loaded sem_seg_head.pixel_decoder.input_proj.1.1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,551] Loaded sem_seg_head.pixel_decoder.input_proj.2.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,551] Loaded sem_seg_head.pixel_decoder.input_proj.2.0.weight, Model Shape: torch.Size([256, 768, 1, 1]) <-> Ckpt Shape: torch.Size([256, 768, 1, 1])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.input_proj.2.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.input_proj.2.1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.input_proj.3.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.input_proj.3.0.weight, Model Shape: torch.Size([256, 768, 3, 3]) <-> Ckpt Shape: torch.Size([256, 768, 3, 3])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.input_proj.3.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.input_proj.3.1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.layer_1.norm.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.layer_1.norm.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.layer_1.weight, Model Shape: torch.Size([256, 256, 3, 3]) <-> Ckpt Shape: torch.Size([256, 256, 3, 3])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.mask_features.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.mask_features.weight, Model Shape: torch.Size([256, 256, 1, 1]) <-> Ckpt Shape: torch.Size([256, 256, 1, 1])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,552] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:38,553] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,554] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:38,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:38,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:38,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:38,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:38,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:38,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,555] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,556] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,556] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,556] Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,556] Loaded sem_seg_head.pixel_decoder.transformer.level_embed, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:38,556] Loaded sem_seg_head.predictor._bbox_embed.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,556] Loaded sem_seg_head.predictor._bbox_embed.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,556] Loaded sem_seg_head.predictor._bbox_embed.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,556] Loaded sem_seg_head.predictor._bbox_embed.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,556] Loaded sem_seg_head.predictor._bbox_embed.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:38,556] Loaded sem_seg_head.predictor._bbox_embed.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:38,556] Loaded sem_seg_head.predictor.bbox_embed.0.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,556] Loaded sem_seg_head.predictor.bbox_embed.0.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,556] Loaded sem_seg_head.predictor.bbox_embed.0.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,556] Loaded sem_seg_head.predictor.bbox_embed.0.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,556] Loaded sem_seg_head.predictor.bbox_embed.0.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:38,556] Loaded sem_seg_head.predictor.bbox_embed.0.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:38,556] Loaded sem_seg_head.predictor.bbox_embed.1.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,556] Loaded sem_seg_head.predictor.bbox_embed.1.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,556] Loaded sem_seg_head.predictor.bbox_embed.1.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,557] Loaded sem_seg_head.predictor.bbox_embed.1.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,557] Loaded sem_seg_head.predictor.bbox_embed.1.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:38,557] Loaded sem_seg_head.predictor.bbox_embed.1.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:38,557] Loaded sem_seg_head.predictor.bbox_embed.2.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,557] Loaded sem_seg_head.predictor.bbox_embed.2.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,557] Loaded sem_seg_head.predictor.bbox_embed.2.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,557] Loaded sem_seg_head.predictor.bbox_embed.2.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,557] Loaded sem_seg_head.predictor.bbox_embed.2.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:38,557] Loaded sem_seg_head.predictor.bbox_embed.2.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:38,557] Loaded sem_seg_head.predictor.bbox_embed.3.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,557] Loaded sem_seg_head.predictor.bbox_embed.3.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,557] Loaded sem_seg_head.predictor.bbox_embed.3.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,557] Loaded sem_seg_head.predictor.bbox_embed.3.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,557] Loaded sem_seg_head.predictor.bbox_embed.3.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:38,557] Loaded sem_seg_head.predictor.bbox_embed.3.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:38,557] Loaded sem_seg_head.predictor.bbox_embed.4.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,557] Loaded sem_seg_head.predictor.bbox_embed.4.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,558] Loaded sem_seg_head.predictor.bbox_embed.4.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,558] Loaded sem_seg_head.predictor.bbox_embed.4.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,558] Loaded sem_seg_head.predictor.bbox_embed.4.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:38,558] Loaded sem_seg_head.predictor.bbox_embed.4.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:38,558] Loaded sem_seg_head.predictor.bbox_embed.5.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,558] Loaded sem_seg_head.predictor.bbox_embed.5.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,558] Loaded sem_seg_head.predictor.bbox_embed.5.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,558] Loaded sem_seg_head.predictor.bbox_embed.5.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,558] Loaded sem_seg_head.predictor.bbox_embed.5.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:38,558] Loaded sem_seg_head.predictor.bbox_embed.5.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:38,558] Loaded sem_seg_head.predictor.bbox_embed.6.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,558] Loaded sem_seg_head.predictor.bbox_embed.6.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,558] Loaded sem_seg_head.predictor.bbox_embed.6.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,558] Loaded sem_seg_head.predictor.bbox_embed.6.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,558] Loaded sem_seg_head.predictor.bbox_embed.6.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:38,558] Loaded sem_seg_head.predictor.bbox_embed.6.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:38,558] Loaded sem_seg_head.predictor.bbox_embed.7.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,559] Loaded sem_seg_head.predictor.bbox_embed.7.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,559] Loaded sem_seg_head.predictor.bbox_embed.7.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,559] Loaded sem_seg_head.predictor.bbox_embed.7.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,559] Loaded sem_seg_head.predictor.bbox_embed.7.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:38,559] Loaded sem_seg_head.predictor.bbox_embed.7.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:38,559] Loaded sem_seg_head.predictor.bbox_embed.8.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,559] Loaded sem_seg_head.predictor.bbox_embed.8.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,559] Loaded sem_seg_head.predictor.bbox_embed.8.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,559] Loaded sem_seg_head.predictor.bbox_embed.8.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,559] Loaded sem_seg_head.predictor.bbox_embed.8.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:38,560] Loaded sem_seg_head.predictor.bbox_embed.8.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:38,560] Loaded sem_seg_head.predictor.class_embed, Model Shape: torch.Size([256, 512]) <-> Ckpt Shape: torch.Size([256, 512])
[2023-06-26 17:34:38,560] Loaded sem_seg_head.predictor.decoder.bbox_embed.0.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,560] Loaded sem_seg_head.predictor.decoder.bbox_embed.0.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,560] Loaded sem_seg_head.predictor.decoder.bbox_embed.0.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,560] Loaded sem_seg_head.predictor.decoder.bbox_embed.0.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,560] Loaded sem_seg_head.predictor.decoder.bbox_embed.0.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:38,560] Loaded sem_seg_head.predictor.decoder.bbox_embed.0.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:38,560] Loaded sem_seg_head.predictor.decoder.bbox_embed.1.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,560] Loaded sem_seg_head.predictor.decoder.bbox_embed.1.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,561] Loaded sem_seg_head.predictor.decoder.bbox_embed.1.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,561] Loaded sem_seg_head.predictor.decoder.bbox_embed.1.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,561] Loaded sem_seg_head.predictor.decoder.bbox_embed.1.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:38,561] Loaded sem_seg_head.predictor.decoder.bbox_embed.1.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:38,561] Loaded sem_seg_head.predictor.decoder.bbox_embed.2.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,561] Loaded sem_seg_head.predictor.decoder.bbox_embed.2.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,561] Loaded sem_seg_head.predictor.decoder.bbox_embed.2.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,561] Loaded sem_seg_head.predictor.decoder.bbox_embed.2.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,561] Loaded sem_seg_head.predictor.decoder.bbox_embed.2.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:38,561] Loaded sem_seg_head.predictor.decoder.bbox_embed.2.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:38,561] Loaded sem_seg_head.predictor.decoder.bbox_embed.3.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,561] Loaded sem_seg_head.predictor.decoder.bbox_embed.3.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,562] Loaded sem_seg_head.predictor.decoder.bbox_embed.3.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,562] Loaded sem_seg_head.predictor.decoder.bbox_embed.3.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,562] Loaded sem_seg_head.predictor.decoder.bbox_embed.3.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:38,562] Loaded sem_seg_head.predictor.decoder.bbox_embed.3.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:38,562] Loaded sem_seg_head.predictor.decoder.bbox_embed.4.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,562] Loaded sem_seg_head.predictor.decoder.bbox_embed.4.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,562] Loaded sem_seg_head.predictor.decoder.bbox_embed.4.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,562] Loaded sem_seg_head.predictor.decoder.bbox_embed.4.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,562] Loaded sem_seg_head.predictor.decoder.bbox_embed.4.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:38,562] Loaded sem_seg_head.predictor.decoder.bbox_embed.4.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:38,562] Loaded sem_seg_head.predictor.decoder.bbox_embed.5.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,562] Loaded sem_seg_head.predictor.decoder.bbox_embed.5.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,563] Loaded sem_seg_head.predictor.decoder.bbox_embed.5.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,563] Loaded sem_seg_head.predictor.decoder.bbox_embed.5.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,563] Loaded sem_seg_head.predictor.decoder.bbox_embed.5.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:38,563] Loaded sem_seg_head.predictor.decoder.bbox_embed.5.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:38,563] Loaded sem_seg_head.predictor.decoder.bbox_embed.6.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,563] Loaded sem_seg_head.predictor.decoder.bbox_embed.6.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,563] Loaded sem_seg_head.predictor.decoder.bbox_embed.6.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,563] Loaded sem_seg_head.predictor.decoder.bbox_embed.6.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,563] Loaded sem_seg_head.predictor.decoder.bbox_embed.6.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:38,563] Loaded sem_seg_head.predictor.decoder.bbox_embed.6.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:38,563] Loaded sem_seg_head.predictor.decoder.bbox_embed.7.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,563] Loaded sem_seg_head.predictor.decoder.bbox_embed.7.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,563] Loaded sem_seg_head.predictor.decoder.bbox_embed.7.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,563] Loaded sem_seg_head.predictor.decoder.bbox_embed.7.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,563] Loaded sem_seg_head.predictor.decoder.bbox_embed.7.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:38,563] Loaded sem_seg_head.predictor.decoder.bbox_embed.7.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:38,564] Loaded sem_seg_head.predictor.decoder.bbox_embed.8.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,564] Loaded sem_seg_head.predictor.decoder.bbox_embed.8.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,564] Loaded sem_seg_head.predictor.decoder.bbox_embed.8.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,564] Loaded sem_seg_head.predictor.decoder.bbox_embed.8.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,564] Loaded sem_seg_head.predictor.decoder.bbox_embed.8.layers.2.bias, Model Shape: torch.Size([4]) <-> Ckpt Shape: torch.Size([4])
[2023-06-26 17:34:38,564] Loaded sem_seg_head.predictor.decoder.bbox_embed.8.layers.2.weight, Model Shape: torch.Size([4, 256]) <-> Ckpt Shape: torch.Size([4, 256])
[2023-06-26 17:34:38,564] Loaded sem_seg_head.predictor.decoder.layers.0.cross_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:38,564] Loaded sem_seg_head.predictor.decoder.layers.0.cross_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:38,564] Loaded sem_seg_head.predictor.decoder.layers.0.cross_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,564] Loaded sem_seg_head.predictor.decoder.layers.0.cross_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,564] Loaded sem_seg_head.predictor.decoder.layers.0.cross_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,564] Loaded sem_seg_head.predictor.decoder.layers.0.cross_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,564] Loaded sem_seg_head.predictor.decoder.layers.0.cross_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,564] Loaded sem_seg_head.predictor.decoder.layers.0.cross_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,564] Loaded sem_seg_head.predictor.decoder.layers.0.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,565] Loaded sem_seg_head.predictor.decoder.layers.0.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:38,565] Loaded sem_seg_head.predictor.decoder.layers.0.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,565] Loaded sem_seg_head.predictor.decoder.layers.0.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:38,565] Loaded sem_seg_head.predictor.decoder.layers.0.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,565] Loaded sem_seg_head.predictor.decoder.layers.0.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,565] Loaded sem_seg_head.predictor.decoder.layers.0.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,565] Loaded sem_seg_head.predictor.decoder.layers.0.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,565] Loaded sem_seg_head.predictor.decoder.layers.0.norm3.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,565] Loaded sem_seg_head.predictor.decoder.layers.0.norm3.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,565] Loaded sem_seg_head.predictor.decoder.layers.0.self_attn.in_proj_bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,565] Loaded sem_seg_head.predictor.decoder.layers.0.self_attn.in_proj_weight, Model Shape: torch.Size([768, 256]) <-> Ckpt Shape: torch.Size([768, 256])
[2023-06-26 17:34:38,565] Loaded sem_seg_head.predictor.decoder.layers.0.self_attn.out_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,565] Loaded sem_seg_head.predictor.decoder.layers.0.self_attn.out_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,565] Loaded sem_seg_head.predictor.decoder.layers.1.cross_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:38,565] Loaded sem_seg_head.predictor.decoder.layers.1.cross_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:38,566] Loaded sem_seg_head.predictor.decoder.layers.1.cross_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,566] Loaded sem_seg_head.predictor.decoder.layers.1.cross_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,566] Loaded sem_seg_head.predictor.decoder.layers.1.cross_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,566] Loaded sem_seg_head.predictor.decoder.layers.1.cross_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,566] Loaded sem_seg_head.predictor.decoder.layers.1.cross_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,566] Loaded sem_seg_head.predictor.decoder.layers.1.cross_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,566] Loaded sem_seg_head.predictor.decoder.layers.1.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,566] Loaded sem_seg_head.predictor.decoder.layers.1.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:38,566] Loaded sem_seg_head.predictor.decoder.layers.1.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,566] Loaded sem_seg_head.predictor.decoder.layers.1.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:38,566] Loaded sem_seg_head.predictor.decoder.layers.1.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,566] Loaded sem_seg_head.predictor.decoder.layers.1.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,566] Loaded sem_seg_head.predictor.decoder.layers.1.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,566] Loaded sem_seg_head.predictor.decoder.layers.1.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,567] Loaded sem_seg_head.predictor.decoder.layers.1.norm3.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,567] Loaded sem_seg_head.predictor.decoder.layers.1.norm3.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,567] Loaded sem_seg_head.predictor.decoder.layers.1.self_attn.in_proj_bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,567] Loaded sem_seg_head.predictor.decoder.layers.1.self_attn.in_proj_weight, Model Shape: torch.Size([768, 256]) <-> Ckpt Shape: torch.Size([768, 256])
[2023-06-26 17:34:38,567] Loaded sem_seg_head.predictor.decoder.layers.1.self_attn.out_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,567] Loaded sem_seg_head.predictor.decoder.layers.1.self_attn.out_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,567] Loaded sem_seg_head.predictor.decoder.layers.2.cross_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:38,567] Loaded sem_seg_head.predictor.decoder.layers.2.cross_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:38,567] Loaded sem_seg_head.predictor.decoder.layers.2.cross_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,567] Loaded sem_seg_head.predictor.decoder.layers.2.cross_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,567] Loaded sem_seg_head.predictor.decoder.layers.2.cross_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,567] Loaded sem_seg_head.predictor.decoder.layers.2.cross_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,567] Loaded sem_seg_head.predictor.decoder.layers.2.cross_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,567] Loaded sem_seg_head.predictor.decoder.layers.2.cross_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,567] Loaded sem_seg_head.predictor.decoder.layers.2.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,567] Loaded sem_seg_head.predictor.decoder.layers.2.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:38,568] Loaded sem_seg_head.predictor.decoder.layers.2.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,568] Loaded sem_seg_head.predictor.decoder.layers.2.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:38,568] Loaded sem_seg_head.predictor.decoder.layers.2.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,568] Loaded sem_seg_head.predictor.decoder.layers.2.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,568] Loaded sem_seg_head.predictor.decoder.layers.2.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,568] Loaded sem_seg_head.predictor.decoder.layers.2.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,568] Loaded sem_seg_head.predictor.decoder.layers.2.norm3.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,568] Loaded sem_seg_head.predictor.decoder.layers.2.norm3.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,568] Loaded sem_seg_head.predictor.decoder.layers.2.self_attn.in_proj_bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,568] Loaded sem_seg_head.predictor.decoder.layers.2.self_attn.in_proj_weight, Model Shape: torch.Size([768, 256]) <-> Ckpt Shape: torch.Size([768, 256])
[2023-06-26 17:34:38,568] Loaded sem_seg_head.predictor.decoder.layers.2.self_attn.out_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,568] Loaded sem_seg_head.predictor.decoder.layers.2.self_attn.out_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,568] Loaded sem_seg_head.predictor.decoder.layers.3.cross_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:38,568] Loaded sem_seg_head.predictor.decoder.layers.3.cross_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:38,568] Loaded sem_seg_head.predictor.decoder.layers.3.cross_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,569] Loaded sem_seg_head.predictor.decoder.layers.3.cross_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,569] Loaded sem_seg_head.predictor.decoder.layers.3.cross_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,569] Loaded sem_seg_head.predictor.decoder.layers.3.cross_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,569] Loaded sem_seg_head.predictor.decoder.layers.3.cross_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,569] Loaded sem_seg_head.predictor.decoder.layers.3.cross_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,569] Loaded sem_seg_head.predictor.decoder.layers.3.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,569] Loaded sem_seg_head.predictor.decoder.layers.3.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:38,569] Loaded sem_seg_head.predictor.decoder.layers.3.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,569] Loaded sem_seg_head.predictor.decoder.layers.3.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:38,569] Loaded sem_seg_head.predictor.decoder.layers.3.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,569] Loaded sem_seg_head.predictor.decoder.layers.3.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,569] Loaded sem_seg_head.predictor.decoder.layers.3.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,570] Loaded sem_seg_head.predictor.decoder.layers.3.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,570] Loaded sem_seg_head.predictor.decoder.layers.3.norm3.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,570] Loaded sem_seg_head.predictor.decoder.layers.3.norm3.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,570] Loaded sem_seg_head.predictor.decoder.layers.3.self_attn.in_proj_bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,570] Loaded sem_seg_head.predictor.decoder.layers.3.self_attn.in_proj_weight, Model Shape: torch.Size([768, 256]) <-> Ckpt Shape: torch.Size([768, 256])
[2023-06-26 17:34:38,570] Loaded sem_seg_head.predictor.decoder.layers.3.self_attn.out_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,570] Loaded sem_seg_head.predictor.decoder.layers.3.self_attn.out_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,570] Loaded sem_seg_head.predictor.decoder.layers.4.cross_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:38,570] Loaded sem_seg_head.predictor.decoder.layers.4.cross_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:38,570] Loaded sem_seg_head.predictor.decoder.layers.4.cross_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,570] Loaded sem_seg_head.predictor.decoder.layers.4.cross_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,570] Loaded sem_seg_head.predictor.decoder.layers.4.cross_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,570] Loaded sem_seg_head.predictor.decoder.layers.4.cross_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,571] Loaded sem_seg_head.predictor.decoder.layers.4.cross_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,571] Loaded sem_seg_head.predictor.decoder.layers.4.cross_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,571] Loaded sem_seg_head.predictor.decoder.layers.4.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,571] Loaded sem_seg_head.predictor.decoder.layers.4.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:38,571] Loaded sem_seg_head.predictor.decoder.layers.4.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,571] Loaded sem_seg_head.predictor.decoder.layers.4.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:38,571] Loaded sem_seg_head.predictor.decoder.layers.4.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,571] Loaded sem_seg_head.predictor.decoder.layers.4.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,571] Loaded sem_seg_head.predictor.decoder.layers.4.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,571] Loaded sem_seg_head.predictor.decoder.layers.4.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,571] Loaded sem_seg_head.predictor.decoder.layers.4.norm3.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,571] Loaded sem_seg_head.predictor.decoder.layers.4.norm3.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,572] Loaded sem_seg_head.predictor.decoder.layers.4.self_attn.in_proj_bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,572] Loaded sem_seg_head.predictor.decoder.layers.4.self_attn.in_proj_weight, Model Shape: torch.Size([768, 256]) <-> Ckpt Shape: torch.Size([768, 256])
[2023-06-26 17:34:38,572] Loaded sem_seg_head.predictor.decoder.layers.4.self_attn.out_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,572] Loaded sem_seg_head.predictor.decoder.layers.4.self_attn.out_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,572] Loaded sem_seg_head.predictor.decoder.layers.5.cross_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:38,572] Loaded sem_seg_head.predictor.decoder.layers.5.cross_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:38,572] Loaded sem_seg_head.predictor.decoder.layers.5.cross_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,572] Loaded sem_seg_head.predictor.decoder.layers.5.cross_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,572] Loaded sem_seg_head.predictor.decoder.layers.5.cross_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,572] Loaded sem_seg_head.predictor.decoder.layers.5.cross_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,572] Loaded sem_seg_head.predictor.decoder.layers.5.cross_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,572] Loaded sem_seg_head.predictor.decoder.layers.5.cross_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,572] Loaded sem_seg_head.predictor.decoder.layers.5.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,572] Loaded sem_seg_head.predictor.decoder.layers.5.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:38,572] Loaded sem_seg_head.predictor.decoder.layers.5.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,572] Loaded sem_seg_head.predictor.decoder.layers.5.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:38,572] Loaded sem_seg_head.predictor.decoder.layers.5.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,572] Loaded sem_seg_head.predictor.decoder.layers.5.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,573] Loaded sem_seg_head.predictor.decoder.layers.5.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,573] Loaded sem_seg_head.predictor.decoder.layers.5.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,573] Loaded sem_seg_head.predictor.decoder.layers.5.norm3.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,573] Loaded sem_seg_head.predictor.decoder.layers.5.norm3.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,573] Loaded sem_seg_head.predictor.decoder.layers.5.self_attn.in_proj_bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,573] Loaded sem_seg_head.predictor.decoder.layers.5.self_attn.in_proj_weight, Model Shape: torch.Size([768, 256]) <-> Ckpt Shape: torch.Size([768, 256])
[2023-06-26 17:34:38,573] Loaded sem_seg_head.predictor.decoder.layers.5.self_attn.out_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,573] Loaded sem_seg_head.predictor.decoder.layers.5.self_attn.out_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,573] Loaded sem_seg_head.predictor.decoder.layers.6.cross_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:38,573] Loaded sem_seg_head.predictor.decoder.layers.6.cross_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:38,573] Loaded sem_seg_head.predictor.decoder.layers.6.cross_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,573] Loaded sem_seg_head.predictor.decoder.layers.6.cross_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,573] Loaded sem_seg_head.predictor.decoder.layers.6.cross_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,573] Loaded sem_seg_head.predictor.decoder.layers.6.cross_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,574] Loaded sem_seg_head.predictor.decoder.layers.6.cross_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,574] Loaded sem_seg_head.predictor.decoder.layers.6.cross_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,574] Loaded sem_seg_head.predictor.decoder.layers.6.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,574] Loaded sem_seg_head.predictor.decoder.layers.6.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:38,574] Loaded sem_seg_head.predictor.decoder.layers.6.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,574] Loaded sem_seg_head.predictor.decoder.layers.6.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:38,574] Loaded sem_seg_head.predictor.decoder.layers.6.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,574] Loaded sem_seg_head.predictor.decoder.layers.6.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,574] Loaded sem_seg_head.predictor.decoder.layers.6.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,574] Loaded sem_seg_head.predictor.decoder.layers.6.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,574] Loaded sem_seg_head.predictor.decoder.layers.6.norm3.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,574] Loaded sem_seg_head.predictor.decoder.layers.6.norm3.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,574] Loaded sem_seg_head.predictor.decoder.layers.6.self_attn.in_proj_bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,574] Loaded sem_seg_head.predictor.decoder.layers.6.self_attn.in_proj_weight, Model Shape: torch.Size([768, 256]) <-> Ckpt Shape: torch.Size([768, 256])
[2023-06-26 17:34:38,574] Loaded sem_seg_head.predictor.decoder.layers.6.self_attn.out_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,575] Loaded sem_seg_head.predictor.decoder.layers.6.self_attn.out_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,575] Loaded sem_seg_head.predictor.decoder.layers.7.cross_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:38,575] Loaded sem_seg_head.predictor.decoder.layers.7.cross_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:38,575] Loaded sem_seg_head.predictor.decoder.layers.7.cross_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,575] Loaded sem_seg_head.predictor.decoder.layers.7.cross_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,575] Loaded sem_seg_head.predictor.decoder.layers.7.cross_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,575] Loaded sem_seg_head.predictor.decoder.layers.7.cross_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,575] Loaded sem_seg_head.predictor.decoder.layers.7.cross_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,575] Loaded sem_seg_head.predictor.decoder.layers.7.cross_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,575] Loaded sem_seg_head.predictor.decoder.layers.7.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,576] Loaded sem_seg_head.predictor.decoder.layers.7.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:38,576] Loaded sem_seg_head.predictor.decoder.layers.7.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,576] Loaded sem_seg_head.predictor.decoder.layers.7.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:38,576] Loaded sem_seg_head.predictor.decoder.layers.7.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,576] Loaded sem_seg_head.predictor.decoder.layers.7.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,576] Loaded sem_seg_head.predictor.decoder.layers.7.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,576] Loaded sem_seg_head.predictor.decoder.layers.7.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,576] Loaded sem_seg_head.predictor.decoder.layers.7.norm3.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,576] Loaded sem_seg_head.predictor.decoder.layers.7.norm3.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,576] Loaded sem_seg_head.predictor.decoder.layers.7.self_attn.in_proj_bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,576] Loaded sem_seg_head.predictor.decoder.layers.7.self_attn.in_proj_weight, Model Shape: torch.Size([768, 256]) <-> Ckpt Shape: torch.Size([768, 256])
[2023-06-26 17:34:38,576] Loaded sem_seg_head.predictor.decoder.layers.7.self_attn.out_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,576] Loaded sem_seg_head.predictor.decoder.layers.7.self_attn.out_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,576] Loaded sem_seg_head.predictor.decoder.layers.8.cross_attn.attention_weights.bias, Model Shape: torch.Size([128]) <-> Ckpt Shape: torch.Size([128])
[2023-06-26 17:34:38,576] Loaded sem_seg_head.predictor.decoder.layers.8.cross_attn.attention_weights.weight, Model Shape: torch.Size([128, 256]) <-> Ckpt Shape: torch.Size([128, 256])
[2023-06-26 17:34:38,576] Loaded sem_seg_head.predictor.decoder.layers.8.cross_attn.output_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,576] Loaded sem_seg_head.predictor.decoder.layers.8.cross_attn.output_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,576] Loaded sem_seg_head.predictor.decoder.layers.8.cross_attn.sampling_offsets.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,577] Loaded sem_seg_head.predictor.decoder.layers.8.cross_attn.sampling_offsets.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,577] Loaded sem_seg_head.predictor.decoder.layers.8.cross_attn.value_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,577] Loaded sem_seg_head.predictor.decoder.layers.8.cross_attn.value_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,577] Loaded sem_seg_head.predictor.decoder.layers.8.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,577] Loaded sem_seg_head.predictor.decoder.layers.8.linear1.weight, Model Shape: torch.Size([2048, 256]) <-> Ckpt Shape: torch.Size([2048, 256])
[2023-06-26 17:34:38,577] Loaded sem_seg_head.predictor.decoder.layers.8.linear2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,577] Loaded sem_seg_head.predictor.decoder.layers.8.linear2.weight, Model Shape: torch.Size([256, 2048]) <-> Ckpt Shape: torch.Size([256, 2048])
[2023-06-26 17:34:38,577] Loaded sem_seg_head.predictor.decoder.layers.8.norm1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,577] Loaded sem_seg_head.predictor.decoder.layers.8.norm1.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,577] Loaded sem_seg_head.predictor.decoder.layers.8.norm2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,577] Loaded sem_seg_head.predictor.decoder.layers.8.norm2.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,577] Loaded sem_seg_head.predictor.decoder.layers.8.norm3.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,577] Loaded sem_seg_head.predictor.decoder.layers.8.norm3.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,577] Loaded sem_seg_head.predictor.decoder.layers.8.self_attn.in_proj_bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])
[2023-06-26 17:34:38,577] Loaded sem_seg_head.predictor.decoder.layers.8.self_attn.in_proj_weight, Model Shape: torch.Size([768, 256]) <-> Ckpt Shape: torch.Size([768, 256])
[2023-06-26 17:34:38,577] Loaded sem_seg_head.predictor.decoder.layers.8.self_attn.out_proj.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,577] Loaded sem_seg_head.predictor.decoder.layers.8.self_attn.out_proj.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,578] Loaded sem_seg_head.predictor.decoder.norm.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,578] Loaded sem_seg_head.predictor.decoder.norm.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,578] Loaded sem_seg_head.predictor.decoder.ref_point_head.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,578] Loaded sem_seg_head.predictor.decoder.ref_point_head.layers.0.weight, Model Shape: torch.Size([256, 512]) <-> Ckpt Shape: torch.Size([256, 512])
[2023-06-26 17:34:38,578] Loaded sem_seg_head.predictor.decoder.ref_point_head.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,578] Loaded sem_seg_head.predictor.decoder.ref_point_head.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,578] Loaded sem_seg_head.predictor.decoder_norm.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,578] Loaded sem_seg_head.predictor.decoder_norm.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,578] Loaded sem_seg_head.predictor.enc_output.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,578] Loaded sem_seg_head.predictor.enc_output.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,578] Loaded sem_seg_head.predictor.enc_output_norm.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,578] Loaded sem_seg_head.predictor.enc_output_norm.weight, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,578] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.ln_final.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,579] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.ln_final.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,579] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:38,579] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])
[2023-06-26 17:34:38,579] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,579] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:38,579] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,579] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,579] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,579] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,579] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,579] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])
[2023-06-26 17:34:38,579] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,579] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])
[2023-06-26 17:34:38,579] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:38,580] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])
[2023-06-26 17:34:38,580] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,580] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:38,580] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,580] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,580] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,580] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,580] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,580] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])
[2023-06-26 17:34:38,580] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,580] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])
[2023-06-26 17:34:38,580] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:38,580] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])
[2023-06-26 17:34:38,580] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,581] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:38,581] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,581] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,581] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,581] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,581] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,581] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])
[2023-06-26 17:34:38,581] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,581] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])
[2023-06-26 17:34:38,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:38,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])
[2023-06-26 17:34:38,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:38,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])
[2023-06-26 17:34:38,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])
[2023-06-26 17:34:38,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:38,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])
[2023-06-26 17:34:38,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:38,582] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])
[2023-06-26 17:34:38,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])
[2023-06-26 17:34:38,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:38,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])
[2023-06-26 17:34:38,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:38,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,583] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])
[2023-06-26 17:34:38,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])
[2023-06-26 17:34:38,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:38,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])
[2023-06-26 17:34:38,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:38,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,584] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])
[2023-06-26 17:34:38,585] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,585] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])
[2023-06-26 17:34:38,585] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:38,585] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])
[2023-06-26 17:34:38,585] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,585] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:38,585] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])
[2023-06-26 17:34:38,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])
[2023-06-26 17:34:38,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:38,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])
[2023-06-26 17:34:38,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:38,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,586] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])
[2023-06-26 17:34:38,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])
[2023-06-26 17:34:38,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:38,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])
[2023-06-26 17:34:38,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:38,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])
[2023-06-26 17:34:38,587] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])
[2023-06-26 17:34:38,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:38,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])
[2023-06-26 17:34:38,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:38,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])
[2023-06-26 17:34:38,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])
[2023-06-26 17:34:38,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])
[2023-06-26 17:34:38,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])
[2023-06-26 17:34:38,588] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,589] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:38,589] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,589] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,589] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,589] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,589] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])
[2023-06-26 17:34:38,589] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])
[2023-06-26 17:34:38,589] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])
[2023-06-26 17:34:38,589] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])
[2023-06-26 17:34:38,589] Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.token_embedding.weight, Model Shape: torch.Size([49408, 512]) <-> Ckpt Shape: torch.Size([49408, 512])
[2023-06-26 17:34:38,589] Loaded sem_seg_head.predictor.lang_encoder.lang_proj, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])
[2023-06-26 17:34:38,589] Loaded sem_seg_head.predictor.lang_encoder.logit_scale, Model Shape: torch.Size([]) <-> Ckpt Shape: torch.Size([])
[2023-06-26 17:34:38,589] Loaded sem_seg_head.predictor.lang_mapper, Model Shape: torch.Size([512, 256]) <-> Ckpt Shape: torch.Size([512, 256])
[2023-06-26 17:34:38,589] Loaded sem_seg_head.predictor.mask_embed.layers.0.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,589] Loaded sem_seg_head.predictor.mask_embed.layers.0.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,589] Loaded sem_seg_head.predictor.mask_embed.layers.1.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,589] Loaded sem_seg_head.predictor.mask_embed.layers.1.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,589] Loaded sem_seg_head.predictor.mask_embed.layers.2.bias, Model Shape: torch.Size([256]) <-> Ckpt Shape: torch.Size([256])
[2023-06-26 17:34:38,590] Loaded sem_seg_head.predictor.mask_embed.layers.2.weight, Model Shape: torch.Size([256, 256]) <-> Ckpt Shape: torch.Size([256, 256])
[2023-06-26 17:34:38,590] $UNUSED$ criterion.empty_weight, Ckpt Shape: torch.Size([134])
[2023-06-26 17:34:38,590] $UNUSED$ sem_seg_head.predictor.lang_encoder.lang_encoder.positional_embedding, Ckpt Shape: torch.Size([18, 512])
[2023-06-26 17:34:38,590] *UNMATCHED* sem_seg_head.predictor.lang_encoder.lang_encoder.positional_embedding, Model Shape: torch.Size([77, 512]) <-> Ckpt Shape: torch.Size([18, 512])
[2023-06-26 17:36:05,213] loss: 3.5798
[2023-06-26 17:36:05,213] cal loss: 172.9758
[2023-06-26 17:36:05,213] cal percent loss: 0.6783
[2023-06-26 17:36:05,213] mass loss: 145.8629
[2023-06-26 17:36:05,214] mass percent loss: 0.6691
[2023-06-26 17:36:05,214] fat loss: 9.9473
[2023-06-26 17:36:05,214] fat percent loss: 0.7833
[2023-06-26 17:36:05,214] carb loss: 11.6871
[2023-06-26 17:36:05,214] carb percent loss: 0.6056
[2023-06-26 17:36:05,214] protein loss: 12.8436
[2023-06-26 17:36:05,214] protein percent loss: 0.7096
[2023-06-26 17:38:27,864] DATA:
  IMGS_DIR: /srv/datasets2/nutrition5k_dataset/imagery/realsense_overhead
  METADATAS_PATH: /srv/datasets2/nutrition5k_dataset/metadata/dish_metadata_cafe1.csv
  NAME: nutrition5k
  SPLITS_TEST_PATH: /srv/datasets2/nutrition5k_dataset/dish_ids/splits/depth_test_ids.txt
  SPLITS_TRAIN_PATH: /srv/datasets2/nutrition5k_dataset/dish_ids/splits/depth_train_ids.txt
EVAL:
  HEIGHT: 384
  WIDTH: 512
MODEL:
  MASK_WEIGHT: 0.5
  NAME: openseed
  PRETRAINED: ''
SAVE_PATH: models/fixed/fpnopenseed_wrot_fc3_nodrop_attninit_warmup_150ep_mltscl.pt
TITLE:
- fpn openseed with aug+rotate fc 3 no dropout freeze new attn initialize with warmup
  150ep multi scale
TRAIN:
  BATCH_SIZE: 32
  CKPT: null
  FINETUNE: false
  LAYERS: 1
  LOSS: multi
  LR: 5.0e-05
  NUM_EPOCHS: 150
  SEED: 12345
  WEIGHT_DECAY: 0.0001

[2023-06-26 17:39:39,826] loss: 0.9930
[2023-06-26 17:39:39,827] cal loss: 41.6487
[2023-06-26 17:39:39,827] cal percent loss: 0.1633
[2023-06-26 17:39:39,827] mass loss: 23.5406
[2023-06-26 17:39:39,827] mass percent loss: 0.1080
[2023-06-26 17:39:39,827] fat loss: 3.3071
[2023-06-26 17:39:39,827] fat percent loss: 0.2604
[2023-06-26 17:39:39,828] carb loss: 4.7514
[2023-06-26 17:39:39,828] carb percent loss: 0.2462
[2023-06-26 17:39:39,828] protein loss: 4.0443
[2023-06-26 17:39:39,828] protein percent loss: 0.2234
[2023-06-26 17:42:03,636] DATA:
  IMGS_DIR: /srv/datasets2/nutrition5k_dataset/imagery/realsense_overhead
  METADATAS_PATH: /srv/datasets2/nutrition5k_dataset/metadata/dish_metadata_cafe1.csv
  NAME: nutrition5k
  SPLITS_TEST_PATH: /srv/datasets2/nutrition5k_dataset/dish_ids/splits/depth_test_ids.txt
  SPLITS_TRAIN_PATH: /srv/datasets2/nutrition5k_dataset/dish_ids/splits/depth_train_ids.txt
EVAL:
  HEIGHT: 352
  WIDTH: 480
MODEL:
  MASK_WEIGHT: 0.5
  NAME: openseed
  PRETRAINED: ''
SAVE_PATH: models/fixed/fpnopenseed_wrot_fc3_nodrop_attninit_warmup_150ep_mltscl.pt
TITLE:
- fpn openseed with aug+rotate fc 3 no dropout freeze new attn initialize with warmup
  150ep multi scale
TRAIN:
  BATCH_SIZE: 32
  CKPT: null
  FINETUNE: false
  LAYERS: 1
  LOSS: multi
  LR: 5.0e-05
  NUM_EPOCHS: 150
  SEED: 12345
  WEIGHT_DECAY: 0.0001

[2023-06-26 17:43:12,167] loss: 1.0174
[2023-06-26 17:43:12,167] cal loss: 43.1884
[2023-06-26 17:43:12,167] cal percent loss: 0.1694
[2023-06-26 17:43:12,167] mass loss: 24.0351
[2023-06-26 17:43:12,167] mass percent loss: 0.1103
[2023-06-26 17:43:12,167] fat loss: 3.3282
[2023-06-26 17:43:12,168] fat percent loss: 0.2621
[2023-06-26 17:43:12,168] carb loss: 4.7882
[2023-06-26 17:43:12,168] carb percent loss: 0.2481
[2023-06-26 17:43:12,168] protein loss: 4.2714
[2023-06-26 17:43:12,168] protein percent loss: 0.2360
[2023-06-26 17:43:56,704] DATA:
  IMGS_DIR: /srv/datasets2/nutrition5k_dataset/imagery/realsense_overhead
  METADATAS_PATH: /srv/datasets2/nutrition5k_dataset/metadata/dish_metadata_cafe1.csv
  NAME: nutrition5k
  SPLITS_TEST_PATH: /srv/datasets2/nutrition5k_dataset/dish_ids/splits/depth_test_ids.txt
  SPLITS_TRAIN_PATH: /srv/datasets2/nutrition5k_dataset/dish_ids/splits/depth_train_ids.txt
EVAL:
  HEIGHT: 320
  WIDTH: 448
MODEL:
  MASK_WEIGHT: 0.5
  NAME: openseed
  PRETRAINED: ''
SAVE_PATH: models/fixed/fpnopenseed_wrot_fc3_nodrop_attninit_warmup_150ep_mltscl.pt
TITLE:
- fpn openseed with aug+rotate fc 3 no dropout freeze new attn initialize with warmup
  150ep multi scale
TRAIN:
  BATCH_SIZE: 32
  CKPT: null
  FINETUNE: false
  LAYERS: 1
  LOSS: multi
  LR: 5.0e-05
  NUM_EPOCHS: 150
  SEED: 12345
  WEIGHT_DECAY: 0.0001

[2023-06-26 17:45:02,286] loss: 1.0057
[2023-06-26 17:45:02,287] cal loss: 42.4569
[2023-06-26 17:45:02,287] cal percent loss: 0.1665
[2023-06-26 17:45:02,287] mass loss: 23.4580
[2023-06-26 17:45:02,287] mass percent loss: 0.1076
[2023-06-26 17:45:02,287] fat loss: 3.2727
[2023-06-26 17:45:02,287] fat percent loss: 0.2577
[2023-06-26 17:45:02,287] carb loss: 4.9101
[2023-06-26 17:45:02,287] carb percent loss: 0.2544
[2023-06-26 17:45:02,287] protein loss: 4.1471
[2023-06-26 17:45:02,287] protein percent loss: 0.2291
[2023-06-26 17:45:34,381] DATA:
  IMGS_DIR: /srv/datasets2/nutrition5k_dataset/imagery/realsense_overhead
  METADATAS_PATH: /srv/datasets2/nutrition5k_dataset/metadata/dish_metadata_cafe1.csv
  NAME: nutrition5k
  SPLITS_TEST_PATH: /srv/datasets2/nutrition5k_dataset/dish_ids/splits/depth_test_ids.txt
  SPLITS_TRAIN_PATH: /srv/datasets2/nutrition5k_dataset/dish_ids/splits/depth_train_ids.txt
EVAL:
  HEIGHT: 288
  WIDTH: 384
MODEL:
  MASK_WEIGHT: 0.5
  NAME: openseed
  PRETRAINED: ''
SAVE_PATH: models/fixed/fpnopenseed_wrot_fc3_nodrop_attninit_warmup_150ep_mltscl.pt
TITLE:
- fpn openseed with aug+rotate fc 3 no dropout freeze new attn initialize with warmup
  150ep multi scale
TRAIN:
  BATCH_SIZE: 32
  CKPT: null
  FINETUNE: false
  LAYERS: 1
  LOSS: multi
  LR: 5.0e-05
  NUM_EPOCHS: 150
  SEED: 12345
  WEIGHT_DECAY: 0.0001

[2023-06-26 17:46:33,287] loss: 1.0133
[2023-06-26 17:46:33,287] cal loss: 42.9347
[2023-06-26 17:46:33,287] cal percent loss: 0.1684
[2023-06-26 17:46:33,288] mass loss: 23.6016
[2023-06-26 17:46:33,288] mass percent loss: 0.1083
[2023-06-26 17:46:33,288] fat loss: 3.3134
[2023-06-26 17:46:33,288] fat percent loss: 0.2609
[2023-06-26 17:46:33,288] carb loss: 4.9829
[2023-06-26 17:46:33,288] carb percent loss: 0.2582
[2023-06-26 17:46:33,288] protein loss: 4.1103
[2023-06-26 17:46:33,288] protein percent loss: 0.2271
[2023-06-26 18:47:28,734] DATA:
  IMGS_DIR: /srv/datasets2/nutrition5k_dataset/imagery/realsense_overhead
  METADATAS_PATH: /srv/datasets2/nutrition5k_dataset/metadata/dish_metadata_cafe1.csv
  NAME: nutrition5k
  SPLITS_TEST_PATH: /srv/datasets2/nutrition5k_dataset/dish_ids/splits/depth_test_ids.txt
  SPLITS_TRAIN_PATH: /srv/datasets2/nutrition5k_dataset/dish_ids/splits/depth_train_ids.txt
EVAL:
  HEIGHT: 256
  WIDTH: 352
MODEL:
  MASK_WEIGHT: 0.5
  NAME: openseed
  PRETRAINED: ''
SAVE_PATH: models/fixed/fpnopenseed_wrot_fc3_nodrop_attninit_warmup_150ep_mltscl.pt
TITLE:
- fpn openseed with aug+rotate fc 3 no dropout freeze new attn initialize with warmup
  150ep multi scale
TRAIN:
  BATCH_SIZE: 32
  CKPT: null
  FINETUNE: false
  LAYERS: 1
  LOSS: multi
  LR: 5.0e-05
  NUM_EPOCHS: 150
  SEED: 12345
  WEIGHT_DECAY: 0.0001

[2023-06-26 18:47:49,678] loss: 1.0098
[2023-06-26 18:47:49,679] cal loss: 42.5066
[2023-06-26 18:47:49,679] cal percent loss: 0.1667
[2023-06-26 18:47:49,679] mass loss: 23.2404
[2023-06-26 18:47:49,680] mass percent loss: 0.1066
[2023-06-26 18:47:49,680] fat loss: 3.3218
[2023-06-26 18:47:49,680] fat percent loss: 0.2616
[2023-06-26 18:47:49,680] carb loss: 4.9412
[2023-06-26 18:47:49,680] carb percent loss: 0.2560
[2023-06-26 18:47:49,680] protein loss: 4.1497
[2023-06-26 18:47:49,680] protein percent loss: 0.2293
